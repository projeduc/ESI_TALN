
@book{2019-jurafsky-martin,
	author = {Jurafsky, Dan and Martin, James H.},
	title = {Speech and Language Processing},
	year = {2019},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}

@book{2018-eisenstein,
	author = {Jacob Eisenstein},
	title = {Natural Language Processing},
	year = {2018},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}


@ARTICLE{1990-deerwester-al,
	author = {Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
	title = {Indexing by latent semantic analysis},
	journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
	year = {1990},
	volume = {41},
	number = {6},
	pages = {391--407}
}

@misc{2013-mikolov-al,
	title={Efficient Estimation of Word Representations in Vector Space},
	author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year={2013},
	note={URL: \url{https://arxiv.org/abs/1301.3781}},
	eprint={1301.3781},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{2014-pennington-al,
	title = "{G}lo{V}e: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@inproceedings{2018-peters-al,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1202",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237",
	abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@misc{2015-kim-al,
	title={Character-Aware Neural Language Models},
	author={Yoon Kim and Yacine Jernite and David Sontag and Alexander M. Rush},
	year={2015},
	eprint={1508.06615},
	note = {URL :  \url{https://arxiv.org/abs/1508.06615}},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{2002-finkelstein-al,
	title = {Placing Search in Context: The Concept Revisited},
	year = {2002},
	author={Lev Finkelstein and Evgeniy Gabrilovich and Yossi Matias and Ehud Rivlin and Zach Solan and Gadi Wolfman and Eytan Ruppin},
	issue_date = {January 2002},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {1},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/503104.503110},
	doi = {10.1145/503104.503110},
	journal = {ACM Trans. Inf. Syst.},
	month = jan,
	pages = {116–131},
	numpages = {16},
	keywords = {invisible web, statistical natural language processing, Search, context, semantic processing}
}

@article{2015-hill-al,
	title = "{S}im{L}ex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
	author = "Hill, Felix  and
	Reichart, Roi  and
	Korhonen, Anna",
	journal = "Computational Linguistics",
	volume = "41",
	number = "4",
	month = dec,
	year = "2015",
	url = "https://www.aclweb.org/anthology/J15-4004",
	doi = "10.1162/COLI_a_00237",
	pages = "665--695",
}

@inproceedings{2013-mikolov-al2,
	title = "Linguistic Regularities in Continuous Space Word Representations",
	author = "Mikolov, Tomas  and
	Yih, Wen-tau  and
	Zweig, Geoffrey",
	booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2013",
	address = "Atlanta, Georgia",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N13-1090",
	pages = "746--751",
}

@article {2017-caliskan-al,
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	number = {6334},
	pages = {183--186},
	year = {2017},
	doi = {10.1126/science.aal4230},
	publisher = {American Association for the Advancement of Science},
	abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs{\textemdash}for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.Science, this issue p. 183; see also p. 133Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/356/6334/183},
	eprint = {https://science.sciencemag.org/content/356/6334/183.full.pdf},
	journal = {Science}
}

@article{2019-turc-al,
	title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
	author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1908.08962v2 },
	year={2019},
	note={URL: \url{https://arxiv.org/abs/1908.08962}}
}

@inproceedings{2019-devlin-al,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{2016-wu-al,
	author    = {Yonghui Wu and
	Mike Schuster and
	Zhifeng Chen and
	Quoc V. Le and
	Mohammad Norouzi and
	Wolfgang Macherey and
	Maxim Krikun and
	Yuan Cao and
	Qin Gao and
	Klaus Macherey and
	Jeff Klingner and
	Apurva Shah and
	Melvin Johnson and
	Xiaobing Liu and
	Lukasz Kaiser and
	Stephan Gouws and
	Yoshikiyo Kato and
	Taku Kudo and
	Hideto Kazawa and
	Keith Stevens and
	George Kurian and
	Nishant Patil and
	Wei Wang and
	Cliff Young and
	Jason Smith and
	Jason Riesa and
	Alex Rudnick and
	Oriol Vinyals and
	Greg Corrado and
	Macduff Hughes and
	Jeffrey Dean},
	title     = {Google's Neural Machine Translation System: Bridging the Gap between
	Human and Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1609.08144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.08144},
	archivePrefix = {arXiv},
	eprint    = {1609.08144},
	timestamp = {Thu, 14 Mar 2019 09:34:18 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2017-vaswani-al,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	archivePrefix = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{1995-miller,
	author = {Miller, George A.},
	title = {WordNet: A Lexical Database for English},
	year = {1995},
	issue_date = {Nov. 1995},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {38},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/219717.219748},
	doi = {10.1145/219717.219748},
	journal = {Commun. ACM},
	month = nov,
	pages = {39–41},
	numpages = {3}
}

@book {2010-ruppenhofer-al,
	author={Josef Ruppenhofer and Michael Ellsworth and Miriam R. L Petruck and Christopher R. Johnson and Collin F. Baker and Jan Scheffczyk},
	title={FrameNet II: Extended Theory and Practice},
	year={2010}, 
	publisher={berkeley university}
}

@article{2012-navigli-ponzetto,
	author = {Roberto Navigli and Simone Paolo Ponzetto},
	title =   {{B}abel{N}et: {T}he Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network},
	journal = {Artificial Intelligence},
	year =    {2012},
	volume = {193},
	pages = {217-250}
}
