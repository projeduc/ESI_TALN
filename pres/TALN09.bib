@book{2019-jurafsky-martin,
	author = {Jurafsky, Dan and Martin, James H.},
	title = {Speech and Language Processing},
	year = {2019},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}

@book{2018-eisenstein,
	author = {Jacob Eisenstein},
	title = {Natural Language Processing},
	year = {2018},
}

@article{2006-Cornish,
	author = {Francis Cornish},
	title = {Relations de cohérence en discours : critères de reconnaissance, caractérisation et articulation cohésion-cohérence},
	journaltitle = {Corela [Online]},
	year = {2006},
	doi = {https://doi.org/10.4000/corela.1456},
}

@inproceedings{2008-prasad-al,
	title = "The {P}enn {D}iscourse {T}ree{B}ank 2.0.",
	author = "Prasad, Rashmi  and
	Dinesh, Nikhil  and
	Lee, Alan  and
	Miltsakaki, Eleni  and
	Robaldo, Livio  and
	Joshi, Aravind  and
	Webber, Bonnie",
	booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
	month = may,
	year = "2008",
	address = "Marrakech, Morocco",
	publisher = "European Language Resources Association (ELRA)",
	url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/754_paper.pdf",
	abstract = "We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus.",
}

@inproceedings{2004-lethanh-al,
	title = "Generating Discourse Structures for Written Text",
	author = "Le Thanh, Huong  and
	Abeysinghe, Geetha  and
	Huyck, Christian",
	booktitle = "{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics",
	month = "aug 23{--}aug 27",
	year = "2004",
	address = "Geneva, Switzerland",
	publisher = "COLING",
	url = "https://www.aclweb.org/anthology/C04-1048",
	pages = "329--335",
}

@inproceedings{2018-wang-al,
	title = "Toward Fast and Accurate Neural Discourse Segmentation",
	author = "Wang, Yizhong  and
	Li, Sujian  and
	Yang, Jingfeng",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D18-1116",
	doi = "10.18653/v1/D18-1116",
	pages = "962--967",
	abstract = "Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance.",
}

@inproceedings{2018-yu-al,
	title = "Transition-based Neural {RST} Parsing with Implicit Syntax Features",
	author = "Yu, Nan  and
	Zhang, Meishan  and
	Fu, Guohong",
	booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
	month = aug,
	year = "2018",
	address = "Santa Fe, New Mexico, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/C18-1047",
	pages = "559--570",
	abstract = "Syntax has been a useful source of information for statistical RST discourse parsing. Under the neural setting, a common approach integrates syntax by a recursive neural network (RNN), requiring discrete output trees produced by a supervised syntax parser. In this paper, we propose an implicit syntax feature extraction approach, using hidden-layer vectors extracted from a neural syntax parser. In addition, we propose a simple transition-based model as the baseline, further enhancing it with dynamic oracle. Experiments on the standard dataset show that our baseline model with dynamic oracle is highly competitive. When implicit syntax features are integrated, we are able to obtain further improvements, better than using explicit Tree-RNN.",
}