{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1f4c19",
   "metadata": {},
   "source": [
    "# Text preprocessing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b214d",
   "metadata": {},
   "source": [
    "## I. Text tokenization\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc5bf2",
   "metadata": {},
   "source": [
    "### I.1. Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7031ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a text written by Mr. Aries.',\n",
       " 'It uses U.S. english to illustrate sentence tokenization.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = 'This is a text written by Mr. Aries. It uses U.S. english to illustrate sentence tokenization.'\n",
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34723d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ce texte est écrit par M. Aries.',\n",
       " \"Il a comme but d'illustrer la segmentation d'un texte en français.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text = \"Ce texte est écrit par M. Aries. Il a comme but d'illustrer la segmentation d'un texte en français.\"\n",
    "fr_sents = sent_tokenize(fr_text, language='french')\n",
    "fr_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab82a1",
   "metadata": {},
   "source": [
    "### I.2. Words tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701b5b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'text',\n",
       " 'written',\n",
       " 'by',\n",
       " 'Mr.',\n",
       " 'Aries',\n",
       " '.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'U.S.',\n",
       " 'english',\n",
       " 'to',\n",
       " 'illustrate',\n",
       " 'word',\n",
       " \"'s\",\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = 'This is a text written by Mr. Aries. It uses U.S. english to illustrate word\\'s tokenization.'\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8756efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ce',\n",
       " 'texte',\n",
       " 'est',\n",
       " 'écrit',\n",
       " 'par',\n",
       " 'M.',\n",
       " 'Aries',\n",
       " '.',\n",
       " 'Il',\n",
       " 'a',\n",
       " 'comme',\n",
       " 'but',\n",
       " \"d'illustrer\",\n",
       " 'la',\n",
       " 'segmentation',\n",
       " \"d'un\",\n",
       " 'texte',\n",
       " 'en',\n",
       " 'français',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text = \"Ce texte est écrit par M. Aries. Il a comme but d'illustrer la segmentation d'un texte en français.\"\n",
    "fr_words = word_tokenize(fr_text, language='french')\n",
    "fr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50414984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "# The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. \n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
    "tokens = TreebankWordTokenizer().tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3871e",
   "metadata": {},
   "source": [
    "### I.3. Other forms of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7969d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbaad242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokens = regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f19bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jus', 'ti', 'fi', 'ca', 'tion']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer\n",
    "SSP = SyllableTokenizer()\n",
    "syllables = SSP.tokenize('justification')\n",
    "syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c58e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokens = tknzr.tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88cb07fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a-little', 'or', 'a-little-bit', 'or', 'a-lot', 'in-spite-of']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')], separator='-')\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokens = tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec2848",
   "metadata": {},
   "source": [
    "## II. StopWords filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c089b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62badcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esw = stopwords.words('english')\n",
    "\n",
    "esw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24261a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'two',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', \n",
    "         'Please', 'buy', 'me', 'two', 'of', 'them',  '.', 'Thanks', '.']\n",
    "filtered = [w for w in words if not w.lower() in esw]\n",
    "\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2427d",
   "metadata": {},
   "source": [
    "## III. Stemming\n",
    "\n",
    "https://www.nltk.org/api/nltk.stem.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b0c86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('elect', 'electr')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "lstemmer = LancasterStemmer()\n",
    "pstemmer = PorterStemmer()\n",
    "\n",
    "word = 'electricity'\n",
    "\n",
    "lstem = lstemmer.stem(word)\n",
    "pstem = pstemmer.stem(word)\n",
    "\n",
    "lstem, pstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42756e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'كلمو'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "word = 'أتكلمونني'\n",
    "stem = stemmer.stem(word)\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e36d4ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArabicStemmer',\n",
       " 'DanishStemmer',\n",
       " 'DutchStemmer',\n",
       " 'EnglishStemmer',\n",
       " 'FinnishStemmer',\n",
       " 'FrenchStemmer',\n",
       " 'GermanStemmer',\n",
       " 'HungarianStemmer',\n",
       " 'ItalianStemmer',\n",
       " 'NorwegianStemmer',\n",
       " 'PorterStemmer',\n",
       " 'PortugueseStemmer',\n",
       " 'RomanianStemmer',\n",
       " 'RussianStemmer',\n",
       " 'SnowballStemmer',\n",
       " 'SpanishStemmer',\n",
       " 'StemmerI',\n",
       " 'SwedishStemmer',\n",
       " '_LanguageSpecificStemmer',\n",
       " '_ScandinavianStemmer',\n",
       " '_StandardStemmer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'demo',\n",
       " 'porter',\n",
       " 'prefix_replace',\n",
       " 're',\n",
       " 'stopwords',\n",
       " 'suffix_replace']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "dir(snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32356239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اتكلم'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import ArabicStemmer\n",
    "stemmer = ArabicStemmer()\n",
    "word = 'أتكلمونني'\n",
    "stem = stemmer.stem(word)\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1513dd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "stem = stemmer.stem('cars')\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aed652",
   "metadata": {},
   "source": [
    "## IV. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667984a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wolv', 'wolf')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = 'wolves'\n",
    "stem = stemmer.stem(word)\n",
    "lemma = lemmatizer.lemmatize(word)\n",
    "\n",
    "stem, lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff1cf7",
   "metadata": {},
   "source": [
    "## V. Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a9951b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalized hamming distance\n",
    "from nltk.metrics.segmentation import ghd\n",
    "\n",
    "# To use traditional hamming\n",
    "d = ghd('010010100110', '010110110100', ins_cost=1.0, del_cost=1.0, shift_cost_coeff=1.0) \n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98bfa08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lavenstein\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "d1 = edit_distance('intention', 'execution', substitution_cost=2)\n",
    "d2 = edit_distance('intention', 'execution', substitution_cost=2, transpositions=True)#Damerau–Levenshtein\n",
    "\n",
    "\n",
    "d1, d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25496831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6833333333333332"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import jaro_similarity\n",
    "\n",
    "d = jaro_similarity('amibe', 'immature')\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4dd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
