% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KodeBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================
\changegraphpath{../img/modeles/}
\chapter{Modèles de langage}

\begin{introduction}[LE\textcolor{white}{S} LANGUES]
	\lettrine{S}{i} nous voulions tester qu'un langage soit bien écrit, nous devrions revenir aux règles de composition de ses phrases.
	Un langage se compose d'un vocabulaire et une grammaire pour construire les phrases.
	Par exemple, nous pourrions trouver un syntagme nominale suivi par un verbe suivi par un syntagme nominale si le verbe soit transitif.
	En statistiques, ces règles peuvent être vues comme des probabilités d'apparition d'un mot sachant qu'un ou plusieurs mots soient apparus avant. 
	Les probabilités estimées à partir d'un corpus d'entraînement sont appelées un modèle de langage. 
	Ce dernier est utile pour estimer le mot suivant sachant une liste de mots passés et aussi pour calculer la probabilité qu'une phrase soit juste.
	Dans ce chapitre, nous allons présenter les modèles de langage traditionnels (N-Grammes), ainsi que ceux basés sur les réseaux de neurones.
\end{introduction} 


Une phrase est bien définie si sa probabilité $P(S) = P(w_1, w_2, ..., w_n) $ égale à $1$. 
Une phrase avec une probabilité plus grande qu'une autre a plus de chance de se produire étant donné le contexte appris. 
Cette hypothèse a plusieurs applications : 
\begin{itemize}
	\item Traduction automatique : en traduisant un texte d'une langue vers une autre, nous pouvons trouver des mots avec plusieurs traductions/sens. 
	En plus, l'ordre des mots n'est pas toujours symétrique. 
	En utilisant un modèle de langage de la langue destinataire, nous pouvons vérifier le choix et l'ordre les plus probables. 
	Exemple, 
	\expword{My tall brother \textrightarrow\ P(Mon grand frère) \textgreater\ P(Mon haut frère)}.
	
	\item Correction des fautes grammaticales : du même, en utilisant un modèle de langage, nous pouvons vérifier qu'un mot n'ait pas de chance de se produire après une séquence de mots. 
	En calculant, par exemple, les différentes probabilités des phrases avec les mots similaires (en terme de distance d'édition) avec le mot erroné, nous pouvons suggérer des corrections.
	Exemple, \expword{P(Un objet qu'on puisse emporter) \textgreater\ P(Un objet qu'ont puisse emporter)}.
	
	\item Reconnaissance de paroles : lorsque les paroles sont transformées en texte, le programme peut mélanger entre les mots ou les expressions proches en prononciation. 
	Nous pouvons guider le choix des mots en utilisant un modèle de langage.
	Exemple, \expword{P(Jeudi matin) \textgreater\ P(Je dis matin)}
\end{itemize}
%
En plus de la probabilité d'une phrase, nous pouvons aussi estimer la probabilité d'occurrence d'un mot en se basant sur les mots précédents $P(w_n | w_1, \ldots, w_{n-1}) $. 
Estimer cette probabilité a plusieurs applications :
\begin{itemize}
	\item Auto-complétion : en se basant sur les mots déjà introduits par l'utilisateur, le programme estime la probabilité de chaque mot du vocabulaire et affiche ceux avec une grande probabilité conditionnelle.
	Exemple, \expword{P(traitement automatique de l'information) \textgreater\ P(traitement automatique de l'eau)}.
	
	\item Génération automatique de textes : en utilisant une représentation interne, nous essayons de générer le texte mot par mot. 
	Le programme utilise cette représentation et un modèle de langage pour apprendre la génération.
\end{itemize}


\section{Modèle N-gramme}

La probabilité d'occurrence d'une phrase est calculée en utilisant la formule des probabilités composées exprimée dans l'équation \ref{eq:ph-prob}.
Par exemple, \expword{P(\text{\textit{je travaille à l'ESI}}) =  P(je) P(travaille | je) P(à | je travaille) $ \ldots $ P(ESI | je travaille à l')}.
Afin d'estimer les probabilités conditionnelles, nous utilisons le maximum de vraisemblance (qui va être présenté juste après).
\begin{equation}\label{eq:ph-prob}
	P(w_1 \ldots w_m) =  P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \ldots P(w_m | w_1, \ldots, w_{m-1})
\end{equation}

Dans un langage naturel, plusieurs phrases possibles peuvent se produire ; nous pouvons même générer une infinité de phrases.
Pour estimer une probabilité conditionnelle avec un historique long (mots précédents), nous devons utiliser un corpus (dataset textuel) très grand. 
Vu que les phrases sont infinies, nous ne pouvons pas représenter toutes les combinaisons possibles. 
Dans ce cas, une solution est de limiter la taille de l'historique. 
Donc, nous essayons d'estimer $P(w_i|w_{i-n+1},\ldots,w_{i-1})$ avec un historique de $n-1$ mots ; 
ce modèle est appelé \keyword[N]{N-gramme}.

\subsection{Formulation}

Étant donné un processus stochastique, ceci vérifie la propriété de Markov si l'état futur ne dépend que de l'état présent. 
Donc, la probabilité d'occurrence d'un mot $w_i$ sachant l'occurrence de plusieurs mots $w_1, \ldots, w_{i-1}$ ne dépend que sur le mot précédent $w_{i-1}$ suivant la propriété de Markov (voir l'équation \ref{eq:markov}). 
Ce modèle est appelé un modèle \keyword[B]{Bi-gramme}.
Ce modèle peut être représenté graphiquement comme un automate à état fini où les mots sont représentés comme des états et les transitions sont représentées par des probabilités.
\begin{equation}
	P(w_i | w_1,\ldots, w_{i-1}) = P(w_i | w_{i-1})
	\label{eq:markov}
\end{equation}

Ce modèle peut être généralisé en considérant $n-1$ mots précédents. 
Dans ce cas, l'équation \ref{eq:markov} sera formulée comme l'équation \ref{eq:markov-ngram}.
Le modèle généralisé est appelé \keyword[N]{N-gramme}.
Les modèles les plus utilisés sont : Uni-gramme ($n=1$), Bi-gramme ($n=2$) et Tri-gramme ($n=3$).
Une compilation des différents N-grammes préparée par Google et distribuée sous une licence open source sous le nom : Google Books Ngram\footnote{Google Books Ngram : \url{https://storage.googleapis.com/books/ngrams/books/datasetsv3.html} [visité le 2021-09-25]}
\begin{equation}
	P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i | w_{i-n+1}, \ldots, w_{i-1})
	\label{eq:markov-ngram}
\end{equation}

Donc, l'équation \ref{eq:ph-prob} qui calcule la probabilité d'une phrase sera reformulée comme indiqué dans l'équation \ref{eq:ph-prob-ngram} en utilisant un modèle \keyword[N]{N-gramme}.
\begin{equation}\label{eq:ph-prob-ngram}
	P(w_1 \ldots w_m) \approx \prod_{i=1}^{m} P(w_i | w_{i-n+1}, \ldots, w_{i-1})
\end{equation}

Étant donné une fonction $C(S)$ qui compte le nombre d'occurrences d'une séquence $S$ dans un corpus, la probabilité conditionnelle est calculée selon le maximum de vraisemblance (voir l'équation \ref{eq:max-vrai}).
Le corpus d'entraînement doit être suffisant afin de capturer toutes les combinaisons possibles. 
En fait, plus $n$ est grand, plus le modèle aura besoin de données.
En observant la formule, nous nous demandons : comment pouvons-nous calculer la probabilité conditionnelle des mots du début et de fin ?
Nous marquons le début et la fin des phrases par \keyword{\textless s\textgreater} et \keyword{\textless/s\textgreater} respectivement (une fois pour les bi-grammes, 2 fois pour les tri-grammes, etc.). 
De cette façon, nous pouvons exprimer la probabilité qu'un mot soit au début ou à la fin de la phrase.
\begin{equation}
	P(w_i | w_{i-n+1},\ldots, w_{i-1}) 
	= \frac{C(w_{i-n+1} \ldots w_{i-1} w_i)}{\sum_j C(w_{j-n+1} \ldots w_{j-1} w_j)}
	= \frac{C(w_{i-n+1} \ldots w_{i-1} w_i)}{C(w_{i-n+1} \ldots w_{i-1})}
	\label{eq:max-vrai}
\end{equation}

Supposons que nous ayons un corpus d'entraînement avec 4 phrases. 
Si nous voulions utiliser un modèle \keyword[B]{Bi-gramme}, nous devrions encercler chaque phrase par ``\textless s\textgreater" et ``\textless/s\textgreater".
Voici l'ensemble des phrases :
\begin{itemize}
	\item \textless s\textgreater un ordinateur peut vous aider \textless/s\textgreater
	\item \textless s\textgreater il veut vous aider \textless/s\textgreater
	\item \textless s\textgreater il veut un ordinateur \textless/s\textgreater
	\item \textless s\textgreater il peut nager \textless/s\textgreater
\end{itemize}
%
Dans ce modèle, nous calculons la probabilité d'occurrence de chaque mot sachant le mot qui le précède. 
Par exemple, $P(peut | il) = \frac{C(il\ peut)}{C(il)} = \frac{1}{3}$.
Dans ce cas, la probabilité d'occurrence de la phrase ``\expword{il peut vous aider}" peut être estimée comme suit :
\[
P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{3/4}
\underbrace{P(peut|il)}_{1/3} 
\underbrace{P(vous|peut)}_{1/2} 
\underbrace{P(aider|vous)}_{2/2}
\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{2/2} = 
%	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
\frac{1}{8}
\]
%
Si, nous essayions d'estimer la probabilité d'occurrence de la phrase ``\expword{il peut aider}", nous aurions $0$. 
Même si la phrase soit juste, nous n'aurions pas suffisamment de données pour entraîner notre modèle à capturer cette forme. 
La probabilité est calculée comme suit : 
\[
P(\text{\textit{\textless s\textgreater il peut aider \textless/s\textgreater}}) = 
\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{3/4}
\underbrace{P(peut|il)}_{1/3} 
\underbrace{P(aider|peut)}_{0/2}
\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{2/2} = 
%	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
0
\]
%
Maintenant, nous essayons d'estimer la probabilité d'occurrence de la phrase : 
``\expword{il peut nous aider}" sachant que le mot ``nous" n'existe pas dans le vocabulaire. 
La probabilité peut être estimée comme suit : 
\[
P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{3/4}
\underbrace{P(peut|il)}_{1/3} 
\underbrace{P(nous|peut)}_{0/0} 
\underbrace{P(aider|nous)}_{0/2}
\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{2/2} = 
%	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
\text{\textit{NaN}}
\]
Les mots qui ne figurent pas dans le vocabulaire sont appelés ``Out-of-vocabulary words". 
Afin de régler ce problème, nous pouvons ajouter un autre mot ``\textless UNK\textgreater" au vocabulaire. 
Pour incorporer ce mot dans le corpus d'entraînement, nous pouvons fixer un vocabulaire et remplacer le reste des mots par ce mot clé, ou en peut remplacer les mots les moins fréquents par ``\textless UNK\textgreater". 

\subsection{Lissage (Smoothing)}

Dans l'exemple précédent, nous avons vu le problème des mots qui n'appartiennent pas au vocabulaire (Ex. ``\expword{nous}") qui résultent à une division par $0$. 
Ce dernier problème peut être résolu en réservant un mot clé pour les mots absents (comme présenté précédemment).
Le problème des n-grammes absents (Ex. ``\expword{peut aider}"), qui résultent à une probabilité de $0$, ne peut pas être résolu par cette technique. 
Une solution de ce problème est d'ajouter plus de données.
Même sans ce problème, si nous utilisions des petits N-grammes, nous pourrions ne pas capturer les dépendances à long terme. 
Exemple, ``\expword{\underline{L'ordinateur} que j'ai utilisé hier à l'ESI pendant la séance du cours \underline{a planté}}". 
Donc, nous sommes obligés à utiliser des N-grammes plus grands. 
Pour entraîner un tel modèle, il nous faut une grande quantité de données. 
Pour une vocabulaire de taille $V$ et N-gramme de taille $N$, nous avons $V^N$ n-grammes possibles.
Une solution à tous cela est d'utiliser la technique de lissage.
L'intuition est d'emprunter une petite portion des probabilités des N-grammes existants pour former une probabilité aux N-grammes absents.

\subsubsection{Lissage de Lidstone/Laplace}

Supposons que notre modèle de langage supporte un vocabulaire de taille $V$ et $n$ grammes.
Afin de soustraire une petite probabilité de chaque N-gramme existant et créer une probabilité pour les N-grammes non existants, nous pouvons modifier la formule du maximum de vraisemblance.
Nous utilisons un paramètre de lissage $\alpha$ comme indiqué par l'équation \ref{eq:lidstone}.
Dans le cas général, ceci est appelé ``lissage de Lidstone". 
Lorsque $\alpha = 1$, il est appelé ``lissage de Laplace". 
Si $\alpha = 0.5$, il est appelé ``loi de Jeffreys-Perks".
\begin{equation}
	P(w_i | w_{i-n+1}, \ldots, w_{i-1}) = \frac{C(w_{i-n+1} \ldots w_{i-1} w_i) + \alpha}{C(w_{i-n+1} \ldots w_{i-1}) + \alpha V}
	\label{eq:lidstone}
\end{equation}

Prenons l'exemple précédent, le vocabulaire est de taille $8$ ($8^2 = 64$ bi-grammes possibles).
Si nous essayions d'estimer la probabilité d'occurrence de la phrase ``\expword{il peut aider}", nous n'aurions pas $0$ même si la séquence ``\expword{peut aider}" ne figure pas dans le corpus.
La probabilité de cette phrase en utilisant un modèle Bi-gramme et un lissage de Laplace est calculée comme suit : 
\[
P(\text{\textit{\textless s\textgreater il peut aider \textless/s\textgreater}}) = 
\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{(3+1)/(4+8)}
\underbrace{P(peut|il)}_{(1+1)/(3+8)} 
\underbrace{P(aider|peut)}_{(0+1)/(2+8)}
\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{(2+1)/(2+8)}
\]
%
Revenons à la phrase : 
``\expword{il peut nous aider}" où le mot ``nous" n'existe pas dans le vocabulaire. 
Nous testons si nous pouvons résoudre le problème avec le lissage de Laplace ; sans utiliser la solution du mot clé réservé pour les mots inconnus.
La probabilité en utilisant un modèle Bi-gramme peut être estimée comme suit : 
\[
P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{(3+1)/(4+8)}
\underbrace{P(peut|il)}_{(1+1)/(3+8)} 
\underbrace{P(nous|peut)}_{(0+1)/(0+8)} 
\underbrace{P(aider|nous)}_{(0+1)/(2+8)}
\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{(2+1)/(2+8)}
\]

\subsubsection{Interpolation}

L'idée de l'interpolation est d'entraîner $n$ modèles de langage : de n-grammes jusqu'à uni-gramme. 
La probabilité de l'interpolation $P_I$ sera calculée en utilisant une composition linéaire entre les probabilités des différents modèles. 
Les paramètres de composition $\lambda_j$ seront estimés en utilisant un corpus de réglage avec la condition $\sum_j \lambda_j = 1$.
Dans l'entraînement, nous essayons de maximiser la probabilité d'interpolation sur ce corpus.
La probabilité d'interpolation pour un modèle Tri-gramme peut être calculée en utilisant l'équation \ref{eq:interpolation}. 
\begin{equation}
	P_{I}(w_i | w_{i-2} w_{i-1}) = 
	\lambda_3 P(w_i | w_{i-2} w_{i-1}) 
	+ \lambda_2 P(w_i | w_{i-1}) 
	+ \lambda_1 P(w_i) 
	\label{eq:interpolation}
\end{equation}

Toujours avec l'exemple précédent, nous essayons d'estimer la probabilité d'occurrence de la phrase ``\expword{il peut aider}"
en utilisant l'interpolation Tri-grammes.
Tout d'abord, nous calculons la probabilité en utilisant chaque modèle n-gramme.
\begin{itemize}
	\item Uni-gramme : 
	$P_1(\text{\textit{\textless s\textgreater il peut aider \textless/s\textgreater}}) = 
	\underbrace{P(il)}_{3/16}
	\underbrace{P(peut)}_{2/16} 
	\underbrace{P(aider)}_{2/16} \approx 0.003$
	
	\item Bi-gramme : déjà calculé ; $P_2(\text{\textit{\textless s\textgreater il peut aider \textless/s\textgreater}})=0$.
	
	\item Tri-gramme : Si la probabilité de cette phrase en utilisant le Bi-gramme est nulle, donc elle l'est aussi avec des modèles de l'ordre supérieure. 
	$P_3(\text{\textit{\textless s\textgreater \textless s\textgreater il peut aider \textless/s\textgreater \textless/s\textgreater}})=0$.
	Ici, nous pouvons tomber sur la division par zéro, mais nous allons considérer la probabilité comme nulle.
\end{itemize}
%
Si nous utilisions les paramètres $\lambda_3=0.7,\, \lambda_2=0.2,\, \lambda_1 = 0.1$, la probabilité de l'interpolation serait $P_I = 0.7 P_3 + 0.2 P_2 + 0.1 P_1 = 0.1 P_1 \approx 0.0003$. 
Bien sûr, cette solution ne résout pas le problème des mots absents.

\subsubsection{Back-off de Katz}

L'idée est d'utiliser la probabilité de l'ordre supérieure $n$ si le n-gramme existe dans le corpus d'entraînement. 
Sinon, on passe à l'ordre suivant $n-1$.
Pour maintenir une distribution correcte des probabilités, nous devons réduire la probabilité de l'ordre supérieure pour avoir une probabilité réduite $P^*$. 
La réduction sera distribuée sur les probabilités des N-grammes de l'ordre inférieur en utilisant une fonction $\alpha$ à base du contexte.
La formule pour calculer la probabilité $P_{BO}$ du Back-off de Katz est indiquée dans l'équation \ref{eq:back-off-katz}.
\begin{equation}
	P_{BO}(w_i | w_{i-n+1}, \ldots, w_{i-1}) = 
	\begin{cases}
	P^*(w_i | w_{i-n+1}, \ldots, w_{i-1}) & \text{si } C(w_{i-n+1}, \ldots, w_{i-1} w_i) > 0 \\
	\alpha(w_{i-n+1}, \ldots, w_{i-1}) P_{BO}(w_{i-n+2}, \ldots, w_{i-1}) & \text{sinon}
	\end{cases}
	\label{eq:back-off-katz}
\end{equation}


%===================================================================================
\section{Modèles neuronaux}
%===================================================================================

Comme nous avons vu, les \keywordpl[N]{N-gramme} ont besoin d'utiliser le lissage afin de prendre en considération des combinaisons absentes dans le corpus d'entraînement. 
Ce modèle ne peut pas capturer les mots similaires (utilisés dans le même contexte ; Ex. synonymes). 
En plus, il ne supporte pas des contextes avec un historique long. 
Les réseaux de neurones semblent avantageux considérant ces problèmes. 
Ils n'ont pas besoin de lissage puisqu'ils généralisent mieux ; ils peuvent donner une petite probabilité aux n-grammes absents. 
Aussi, ils peuvent apprendre des représentations proches pour les mots similaires (qui sont dans le  même contexte). 
A cause de leur capacité à généraliser, ils peuvent supporter un contexte plus grand. 
Cependant, ces modèles sont plus lents à entraîner.

\subsection{Réseau de neurones à propagation avant}

Un réseau de neurones à propagation avant est la forme traditionnelle : une couche d'entrée, des couches cachées et une couche de sortie. 
L'idée est de choisir le nombre des n-grammes $n$. 
La couche d'entrée est alimentée par les $n-1$ mots précédents afin d'estimer le mot $n$ dans la couche de sortie. 
Chaque mot est représenté sous forme d'un vecteur de taille $V$ (taille de vocabulaire) où toutes les positions ont un zéro sauf la position réservée pour ce mot. 
Cette représentation est appelée ``\keyword[O]{One-Hot} représentation". 
En sortie, nous aurons un vecteur avec des probabilités ; le mot dont la position ait la plus grande probabilité est celui choisi. 

Ici, nous allons présenter le modèle de \citet{2003-bengio-al} illustré par la figure \ref{fig:bengio-l}.
Après choisir la taille $n$ du modèle, les mots $w_{i-n+1}, \ldots, w_{i-1}$ sont représentés sous forme des vecteurs \keyword[O]{One-Hot} $h_{i-n+1}, \ldots, h_{i-1}$. 
Chacun de ces vecteur est passé par une couche cachée de taille $d$ ; donc nous aurons un vecteur de taille $d$ pour chaque mots de ces $n-1$.
Ce vecteur est appelé \keyword[E]{embedding} (sera discuté en détail dans le chapitre de sémantique lexicale). 
En fusionnant les vecteurs, nous aurons un seul vecteur de contexte $m \in \mathbb{R}^{(n-1) d}$.
Ce vecteur va être passé par deux blocs en parallèle :
\begin{itemize}
	\item Une couche cachée avec les paramètres $A \in \mathbb{R}^{(n-1) \times d \times V}$ (poids) et $b \in \mathbb{R}^{V}$. 
	La somme pondérée résulte à un vecteur de taille $V$. 
	\item Une couche cachée avec les paramètres $T \in \mathbb{R}^{(n-1) \times d \times H}$ suivie par une fonction ``Tanh", ce qui va générer un vecteur de taille $H$. 
	Ce dernier passe par une autre couche avec les paramètres $W \in \mathbb{R}^{V \times H}$, ce qui génère un vecteur de taille $V$. 
\end{itemize}
Les deux vecteurs sont additionnés élément par élément pour avoir un seule vecteur de taille $V$. 
Pour avoir des probabilités avec une somme égale à $1$, nous passons ce vecteur par une fonction ``softmax". 
L'architecture de ce modèle peut être exprimé par l'équation \ref{eq:bengio}.
\begin{equation}
	P(.|h_1,\ldots, h_{n-1}) = 
	Softmax \left(
	(b + m A) 
	+ 
	W\ \tanh(u + m T)
	\right)
	\label{eq:bengio}
\end{equation}

\begin{figure}[ht]
	\centering
	\hgraphpage[0.6\textwidth]{fw-model.pdf}
	\caption[Modèle de langage à base des réseaux de neurones à propagation avant]{Représentation du modèle proposé par \citet{2003-bengio-al}\label{fig:bengio-l}}
\end{figure}

\subsection{Réseaux de neurones récurrents}

Certes, les modèles basés sur les réseaux de neurones à propagation avant sont avantageux par rapport aux \keywordpl[N]{N-gramme}.
Mais, ils ne supportent pas des longueurs variables des contextes.
Par contre, les réseaux de neurones récurrents ont la capacité d'estimer un mot dans la position $i$ sachant tous les mots passés.
Ici, nous allons présenter le modèle de \citet{2010-mokolov-al} qui se base sur le réseau de Elman.

La figure \ref{fig:mokolov} représente un exemple d'exécution du modèle proposé par \citet{2010-mokolov-al}.
Le réseau récurrent a une couche d'entrée $x$, une couche cachée $s$ (l'état) avec la fonction ``sigmoid" et une couche de sortie $y$ avec une fonction ``softmax".
Dans un instant $t$, l'entrée $x$ est composée du mot $w_t \in \mathbb{N}^{V}$ encodé en utilisant One-Hot et le contexte précédent $s_{t-1} \in \mathbb{R}^{H}$ (équation \ref{eq:mokolov1}). 
L'entrée $x_t$ est passée par la couche cachée ayant des paramètres $W \in \mathbb{R}^{(H+V)\times H}$ pour avoir un nouveau contexte $s_t$ (équation \ref{eq:mokolov2}). 
Le contexte $s_t$ est passé vers l'état suivant $t+1$ et il est utilisé pour estimer le mot suivant dans l'état actuel.
Pour ce faire, il passe par une couche de sortie ayant des paramètres $U \in \mathbb{R}^{H\times V}$, ce qui génère un vecteur de taille $V$ qui est passé par une fonction ``softmax" (équation \ref{eq:mokolov3}).
%
\begin{align}
	x_t = s_{t-1} \bullet m_t \label{eq:mokolov1}\\
	s_t = \sigma(x_t W) \label{eq:mokolov2}\\
	y_t = softmax(s_t U) \label{eq:mokolov3}
\end{align}

\begin{figure}[ht]
	\centering
	\hgraphpage[0.6\textwidth]{rnn-model.pdf}
	\caption[Modèle de langage à base des réseaux de neurones récurrents]{Modèle de langage à base des réseaux de neurones récurrents proposé par \citet{2010-mokolov-al}\label{fig:mokolov}}
\end{figure}

Il ne faut pas oublier d'utiliser le mot clé ``\textless UNK\textgreater" afin d'entraîner le modèle à prendre en considération les mots inconnus. 
Le problème avec cette architecture est que le modèle puisse arrêter à apprendre avec un contexte à long terme (problème de disparition des gradients).
Une solution est d'utiliser des architectures plus avancées comme \keyword[L]{LSTM} et \keyword[G]{GRU}. 
Ceci est dit, le problème de disparition des gradients existe toujours. 
Une solution technique est de fixer un nombre maximum des états.

%===================================================================================
\section{Évaluation}
%===================================================================================

Qu'est ce que rend un modèle de langage plus bon qu'un autre ? 
Il faut avoir une méthode pour comparer les deux. 
Pour ce faire, il existe deux approches : 
	\begin{itemize}
		\item \optword{Évaluation extrinsèque} : ici, nous voulons tester l'effet d'un modèle de langage sur une autre tâche. 
		Nous évaluons la tâche en utilisant plusieurs modèles de langage pour choisir celui qui donne des meilleurs résultats.
		Exemple, ``\expword{La qualité de traduction automatique en utilisant ce modèle}". 
		Dans cet exemple, nous essayons de choisir le modèle de langue le plus adéquat pour la tâche de traduction automatique.
		Bien sûr, l'évaluation dans ce cas est vraiment coûteuse vu que nous voulions entraîner le système de traduction à chaque fois que nous modifiions le modèle de langage.
		
		\item \optword{Évaluation intrinsèque} : ici, nous voulons tester la capacité du modèle à représenter le langage. 
		Étant donné deux modèles de langage entraînés sur le même corpus d'entraînement, nous utilisons un autre de test pour tester la capacité de représentation.
		La méthode la plus utilisée dans ce cas est \keyword{la perplexité}.
		Il faut savoir que le fait d'être un modèle représentatif ne garantit pas d'avoir une bonne performance dans une tâche donnée.
	\end{itemize}


La perplexité est une mesure intrinsèque qui vise à tester la qualité de prédiction d'un modèle sur un corpus de test (non vu par ce modèle). 
Étant donné un corpus de test avec la taille $N$, nous ajoutons les marqueurs de début et de fin pour toutes les phrases ; La perplexité traite le corpus comme étant une seule chaîne. 
Le but est de calculer la probabilité d'occurrence du texte (la totalité) en utilisant le modèle de langage. 
Cette probabilité est inversée puis passée par une racine d'ordre $N$ comme indiqué dans l'équation \ref{eq:perplexite}.
Dans ce cas, un modèle ayant une perplexité minimale est le meilleur.
%
\begin{align}
	PP(w) & = \sqrt[N]{\frac{1}{P(w_1 w_2 \ldots w_N)}} \nonumber\\
	 & = \sqrt[N]{\prod\limits_{i=1}^{N}\frac{1}{P(w_i | w_1 \ldots w_{i-1})}} \label{eq:perplexite}
\end{align}



\begin{discussion}

Un langage se définit un vocabulaire et une grammaire afin de composer des phrases. 
Les règles de la grammaire sont définies par des linguistes. 
Des fois, il est vraiment difficile de définir ces règles, surtout si elles se basent sur des exceptions des mots. 
Par exemple, un verbe transitif qui n'accepte pas des mots comme complément d'objet directe puisque la phrase n'aura aucun sens (\expword{J'ai mangé le ciel}). 
Ce dernier exemple peut être réglé si nous avons des statistiques sur le contexte des mots dans un langage : quel mot peut se produire en voisinage d'un autre ? et à quel fréquence ?
Cette représentation est appelée : un modèle de langage ; déjà présenté dans ce chapitre. 

Nous avons vu que le modèle de langage se base sur le vocabulaire appris à partir du corpus d'entraînement. 
Ce que nous n'avons pas discuté est le sens du vocabulaire : que veut-on dire par vocabulaire ?
Nous avons vu dans le chapitre précédent que nous puissions générer des mots à partir d'autres (par exemple, la conjugaison).
En réalité, nous pouvons entraîner le modèle avec toutes les formes possibles. 
De cette façon, nous pouvons représenter le fait que le mot ``étudiez" ne peut pas venir après le mot ``je".
En quelque sorte, nous essayons d'apprendre la syntaxe et la lexique en parallèle. 
Mais, ceci pose un problème dans les langues fortement flexionnelles ; nous aurons un vocabulaire gigantesque.
Théoriquement, ceci ne pose pas de problème puisque le vocabulaire reste toujours un ensemble fini (mis à part l'évolution de la langue : ajout de mots à la lexique). 
Mais pratiquement, plus le vocabulaire est grand, plus la tâche est couteuse. 
Dans ce cas, le traitement prend plus de temps supposant que nous nous disposons d'une mémoire suffisante pour représenter tous les mots. 
Aussi, nous devons avoir un grand corpus afin de capturer toutes les variations morphologiques de tous les mots. 
Une des solutions est d'appliquer une sorte de radicalisation, en séparant la racine et le suffixe. 
Tous les deux seront considérés comme des mots à part. 
De cette manière, nous réduisons la taille  du vocabulaire et nous apprenons la formation des mots au même temps. 

Dans ce chapitre, nous avons présenté les modèles de langage en prenant les mots comme unité. 
En réalité, les modèles de langage peuvent être entraînés sur des caractères. 
Parmi les applications de ce genre de modèles est la détection des langues, surtout pour celles qui utilisent le même système d'écriture. 
Supposons que nous voulons détecter les langues : français, anglais et espagnol. 
Dans ce cas, nous entraînons trois modèles de langage pour chacune de ces langues. 
Étant donné une phrase, nous essayons d'estimer les trois probabilités (niveau caractère) et considérer le modèle qui maximise la probabilité. 
Un modèle de langage peut être entraîné sur des séquences d'ADN. 
Dans ce cas, le vocabulaire sera : ``A", ``T", ``C", ``G" et ``U". 
Parmi ses applications : la détection de l'espèce. 
\end{discussion}

%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%=====================================================================
