% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KodeBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================
\changegraphpath{../img/basique/}
\chapter{Traitements basiques du texte}

\begin{introduction}[L\textcolor{white}{E}S LANGUES]
%	\lettrine{A}{fin} de traiter un langage, nous commençons par sa plus petite unité : le graphème. 
	\lettrine{E}{n} traitant un langage, nous commençons par sa plus petite unité : le graphème. 
	En informatique, les graphèmes plus les ponctuations et les espacements sont appelés caractères. 
	Il existe des opérations qui nous permet de chercher et de comparer les chaînes de caractères.
	En combinant les caractères, nous allons avoir un texte avec des phrases et des mots. 
	Étant donné un texte, nous devons pouvoir le segmenter en phrases et une phrase en mots. 
	Ces derniers peuvent être inutiles pour certains tâches ; comme les prépositions dans la recherche d'information. 
	Donc, il faut les filtrer avant d'appliquer cette tâche. 
	Aussi, les mots peuvent prendre plusieurs variations morphologiques. 
	Si nous voulions traiter une seule variation, il faudrait les normaliser. 
	En plus, nous devons pouvoir générer ces variations ou passer d'une variation à une autre étant donné une forme d'un mot. 
	Ce chapitre résume quelques tâches du niveau morphologique des langages (analyse lexicale).
\end{introduction} 

D'après Larousse, un caractère est définie dans l'informatique et télécommunications comme : ``\textit{Tout symbole (chiffre, lettre de l'alphabet, signe de ponctuation, etc.) employé pour représenter des données en vue de leur traitement ou de leur transmission.}".
Un mot est composé de plusieurs caractères suivant un langage régulier. 
Une phrase, à son tour, se compose de plusieurs mots suivant un langage hors contexte (dans la plupart des langues). 
Cette dernière proposition concerne le niveau syntaxique.
Pour l'instant, nous nous intéressons par le niveau morphologique ou ce qu'on appelle l'analyse lexicale.
Les points traités dans ce chapitre sont les suivants : 
\begin{itemize}
	\item La recherche dans le texte en utilisant les expressions régulières. 
	Parmi les applications : l'extraction des données (dates, numéros de téléphones, adresses émail, etc.).
	
	\item La comparaison entre les chaînes de caractères en utilisant la distance d'édition. 
	Parmi ses applications : la correction d'orthographe et la recherche approximative.
	
	\item La segmentation du texte en phrases et les phrase en mots. 
	
	\item La normalisation du texte afin de diminuer les variations d'un mot.
	
	\item La formation des mots dans les langues synthétiques et l'opération inverse.
\end{itemize}

\section{Traitements sur les caractères}

Ici, nous allons considérer un texte comme une séquence de caractères. 
D'une manière plus formelle, un texte peut être composé en utilisant un automate à états finis où le vocabulaire est l'ensemble des caractères. 
Pour chercher des sous-chaînes de caractères dans un texte, nous pouvons utiliser les expressions régulières reconnaissant des langages de types 3 (langages réguliers) dans la hiérarchie de Chomsky. 
La recherche des séquences dans un texte nous permet de les remplacer ou de les séparer (ex. \expword{séparation des mots}).
Un autre type de recherche est la recherche approximative : chercher des parties presque similaires à une chaîne donnée. 
Pour ce faire, nous devons pouvoir comparer deux chaînes de caractères en mesurant la différence entre les deux. 
Une des techniques utilisées est la distance d'édition.

\subsection{Expressions régulières}

Une expression régulière, appelée \keyword[R]{RegEx}, est une séquence de caractères spécifiant un motif (pattern) de recherche.
Plusieurs langages de programmation fournissent la capacité de recherche et de remplacement en utilisant les expressions régulières. 
Afin de l'utiliser pour la recherche, une expression régulière est transformée à un automate à états finis (AEF) qui est transformé à son tour à un AEF déterministe.
Un point dans une expression régulière représente n'importe quel caractère.
Si nous voudrions rechercher un point et pas n'importe quel caractère, nous devons ajouter un antislash avant le point.
Nous pouvons concevoir une expression régulière complexe en composant plusieurs expressions régulières simples.
Ces dernières sont décrites dans le tableau \ref{fig:exp-reg} avec leurs sens et des exemples.

\begin{table}[ht]
	\begin{tabular}{p{.1\textwidth}p{.34\textwidth}p{.46\textwidth}}
		\hline\hline
		\textbf{ER} & \textbf{Sens} & \textbf{Exemple} \\
		\hline
		
		. & n'importe quel caractère & /beg.n/ : \expword{I \underline{begun} at the \underline{begin}ning} \\
		
		\empty [aeuio] & caractères spécifiques & /[Ll][ae]/ : \expword{\underline{Le} chat mange \underline{la} sourie} \\
		
		\empty [a-e] & plage de caractères & /[A-Z]../ : \expword{\underline{J'a}i vu \underline{Kar}im} \\
		
		\empty [\textasciicircum aeuio] & exclure des caractères & /[\textasciicircum A-Z]a./ : \expword{J\underline{'ai} vu Karim} \\
		
		c? & un ou zéro & /colou?r/ : \expword{It is \underline{colour} or \underline{color}} \\
		
		c* & zéro ou plus & /No*n/ : \expword{\underline{Nn}! \underline{Non}! \underline{Nooooooon}!} \\
		
		c+ & un ou plus & /No+n/ : \expword{Nn! \underline{Non}! \underline{Nooooooon}!} \\
		
		c\{n\} & n occurrences & /No\{3\}n/ : \expword{Nn! Non! Noon! \underline{Nooon}!} \\
		
		c\{n,m\} & de n à m occurrences & /No\{1,2\}n/ : \expword{Nn! \underline{Non}! \underline{Noon}! Nooon!} \\
		
		c\{n,\} & au moins n occurrences & /No\{2,\}n/ : \expword{Nn! Non! \underline{Noon}! \underline{Nooon}!} \\
		
		c\{,m\} & au plus m occurrences & /No\{,2\}n/ : \expword{\underline{Nn}! \underline{Non}! \underline{Noon}! Nooon!} \\
		
		\hline 
		
		\textbackslash d & [0-9] & /\textbackslash d\{2,\}/ : \expword{L'annee \underline{1962}}\\
		
		\textbackslash D & [\textasciicircum 0-9] & /\textbackslash D\{2,\}/ : \expword{\underline{L'annee }1962}\\
		
		\textbackslash w & [a-zA-Z0-9\_] & /\textbackslash w\{2,\}/ : \expword{L'\underline{annee} 1962}\\
		
		\textbackslash W & [\textasciicircum \textbackslash w] & /\textbackslash W\{2,\}/ : \expword{L'annee\underline{ 1962}}\\
		
		\textbackslash s & [ \textbackslash r\textbackslash t\textbackslash n\textbackslash f] & /\textbackslash s+/ : \expword{L'annee\underline{ }1962\underline{ }}\\
		
		\textbackslash S & [\textasciicircum \textbackslash s] & /\textbackslash S+/ : \expword{\underline{L'annee} \underline{1962}}\\
		
		\hline 
		
		( ) & le groupement & /(bla)+/ : \expword{Ceci est du \underline{blabla}}\\
		
		\textbar & la disjonction & /continu(er\textbar ation)/ : \expword{Je continue la \underline{continuation}} \\
		
		\textasciicircum  & début du texte & /\textasciicircum K/ :  \expword{\underline{K}ill Karim}\\
		
		\$ & fin du texte & /\textbackslash .[\textasciicircum .]+\$/ :  \expword{fichier.tar\underline{.gz}}\\
		
		\hline\hline
	\end{tabular}

	\caption{Expressions régulières\label{fig:exp-reg}}
\end{table}

La plus fréquente utilisation des expressions régulières est la recherche des sous-chaînes de caractères dans un grand texte.
La plupart des éditeurs de textes et des langages de programmation fournissent des mécanismes pour utiliser les expressions régulières.
Utiliser des motifs (patterns) pour chercher des chaînes de caractères rend les expressions régulières un outil très puissant pour l'extraction de données.
Par exemple, nous pouvons utiliser les expressions régulières pour extraire les émails à partir des blogs et des réseaux sociaux.
Prenons un exemple d'une expression régulière \expword{/[a-zA-Z]\textbackslash w*(@| at )[a-zA-Z]\textbackslash w+\textbackslash .[a-zA-Z]\{2,3\}/} pour chercher des adresses mail.
La partie \expword{/[a-zA-Z]/} garantie que le nom d'utilisateur et le nom du domaine commencent par une lettre.
Le nom d'utilisateur accepte au moins un caractère puisque la première lettre est suivie par \expword{/\textbackslash w*/} qui veut dire zéro à plusieurs caractères de type lettre, chiffre ou souligné.
Le nom du domaine dans cette expression régulière accepte au moins deux caractères puisque la première lettre est suivie par \expword{/\textbackslash w+/} qui veut dire un à plusieurs caractères de type lettre, chiffre ou souligné.
Le nom d'utilisateur et le nom du domaine sont séparés soit par un ``\expword{@}" ou par ``\expword{ at }" ; ce qui est indiqué par \expword{/(@| at )/}.
L'extension du domaine doit toujours être précédée par un point ; ici, nous utilisons \expword{/\textbackslash ./} pour indiquer qu'il s'agit du caractère point.
Sinon, si nous utilisions \expword{/./}, cela représentait n'importe quel caractère.
L'extension est composée de deux à trois lettres.
Voici quelques émails que nous puissions extraire en utilisant cette expression régulière : 
``\expword{ab\_aries@esi.dz}", ``\expword{ab\_ARIES@eSi.dz}", ``\expword{ab\_aries at esi.dz}", ``\expword{a@es.edu}", ``\expword{a1@e2s.edu}".


Afin de capturer une chaîne de caractère, nous utilisons le regroupement avec le numéro du groupe.
Le numéro du groupe ``N" doit être précédé par un caractère spécial , souvent ``\$" ou ``\textbackslash".
Un exemple de remplacement de texte dans un éditeur de texte (``kate" dans notre cas) est illustré dans la figure \ref{fig:kate_regex}.
Ici, nous cherchons le mot ``chat" pour le remplacer par le mot ``chien". 
Mais, nous voulons garder ``ch" comme c'est partagé entre les deux mots.
Dans ce cas, nous capturons la chaîne ``ch" pour la garder et remplacer le reste : ``at" par ``ien".

\begin{figure}[ht]
	\centering
	\hgraphpage[.75\textwidth]{expreg_exp_kate.png}
	\caption[Exemple de remplacement par RegEx dans l'éditeur de texte ``kate".]{Exemple de remplacement par expressions régulières dans l'éditeur de texte ``kate".}
	\label{fig:kate_regex}
\end{figure}


Peut-être l'utilité du regroupement n'est pas claire avec l'exemple précédent puisque nous avons une seule chaîne à capturer.
Si nous avions une disjonction, ce serait plus intéressant.
Prenons l'exemple : ``\expword{J'aime les chats. Dans la cuisine, il y a un rat.}". 
Si nous remplacions la chaîne ``at" de ``chat" et de ``rat" par ``ien", nous aurions le texte suivant : 
``\expword{J'aime les chiens. Dans la cuisine, il y a un rien.}".
Dans ce cas, nous devons tester les mots ``chat" et ``rat" pour remplacer seulement la dernière partie et garder la première intacte.
Plusieurs langages de programmation fournissent le remplacement des chaînes de caractères en utilisant les \keyword[R]{RegEx}.
Dans Javascript, les chaînes de caractères sont attribuées une méthode ``replace" pour remplacer une de leurs parties par une autre chaîne. 
Si nous voulions remplacer toutes les occurrences, nous devrions utiliser le flag ``g" après l'expression régulière.
Pour récupérer le groupe capturé, nous utilisons ``\$" suivi par l'ordre du groupe.

\begin{lstlisting}[language={[KB]Javascript}, style=codeStyle]
let orig = "J'aime les chats. Dans la cuisine, il y a un rat.";
let remp = orig.replace(/(ch|r)at/g, "$1ien");
\end{lstlisting}

Pour toute utilisation des expressions régulières en Python, nous devons faire appel au module ``re".
Une expression régulière est une chaîne de caractères précédée par l'indicateur ``r".
Pour récupérer le groupe capturé, nous utilisons ``\textbackslash".

\begin{lstlisting}[language=Python, style=codeStyle]
import re
orig = "J'aime les chats. Dans la cuisine, il y a un rat."
remp = re.sub(r'(ch|r)at', r'\1ien', orig)
\end{lstlisting}


\subsection{Distance d'édition}

Des fois, lorsque nous rédigeons en utilisant nos ordinateurs, nous faisons des erreurs de frappe ; nous pouvons insérer un caractère de plus, dupliquer un caractère, etc. 
Une telle opération est appelée ``opération d'édition". 
Nous pouvons comparer entre deux chaînes de caractères en comptant le nombre des opérations d'édition pour passer d'une chaîne originale à une autre modifiée.
Les différentes opérations d'édition sont les suivantes (où $X$ est l'ensemble des caractères du langage) :
%
\begin{itemize}
	\item \optword{Insertion} : insertion d'un caractère dans une chaîne 
	($uv \rightarrow uxv \,/\, u, v \in X^*;\, uv \in X^+;\, x \in X$).
	Par exemple, \expword{courir $ \rightarrow $ courrir, entraînement $ \rightarrow $ entraînnement}.
	
	\item \optword{Suppression} : suppression d'un caractère d'une chaîne
	($uxv \rightarrow uv \,/\, u, v \in X^*;\, uv \in X^+;\, x \in X$).
	Par exemple, \expword{héros $ \rightarrow $ héro, meilleur $ \rightarrow $ meileur}.
	
	\item \optword{Substitution} : substitution d'un caractère par un autre
	($uxv \rightarrow uyv \,/\, u, v \in X^*;\, x, y \in X;\, x \ne y$).
	Par exemple, \expword{cela $ \rightarrow $ celà, croient $ \rightarrow $ croyent }
	
	\item \optword{Transposition} : changement de l'ordre de deux caractères
	($uxwyv \rightarrow uywxv \,/\, u, v, w \in X^*;\, x, y \in X;\, x \ne y$).
	Par exemple, \expword{cueillir $ \rightarrow $ ceuillir}.
\end{itemize}

Savoir le nombre des opérations d'édition nous permet de comparer deux chaînes de caractères. 
Il existe plusieurs exemples de l'utilisation de la distance d'édition (nombre des modifications) :
\begin{itemize}
	\item \optword{Révision des fichiers} : par exemple, la commande Unix ``\expword{diff}" qui compare deux fichiers.
	\item \optword{Correction d'orthographe} : suggérer des corrections possibles d'une faute (Ex. \expword{Hunspell}).
	\item \optword{Détection du plagiat} : ici, nous utilisons des mots à la place des caractères.
	\item \optword{Filtrage de spam} : parfois, les spammeurs commettent des fautes d'orthographe intentionnellement pour tromper l'outil de détection de spam.
	\item \optword{Bio-informatique} : quantification de la similarité entre deux séquences d'ADN.
\end{itemize}

\subsubsection{Distance de Hamming}

Cette distance permet seulement la substitution.
Les chaînes doivent être de la même longueur.
Parmi ses utilisations, nous pouvons citer la détection des erreurs de transmission de données.\newline
Par exemple, \expword{D(010\underline{0}101\underline{0}01\underline{1}0, 010\underline{1}101\underline{1}01\underline{0}0) = 3}.
Pour calculer la distance, nous comparons les deux chaînes caractère par caractère pour avoir le nombre des caractères différents (voir l'algorithme \ref{algo:hamming}).

\begin{algorithm}[ht]
	\KwData{orig, modif}
	\KwResult{distance: entier}
	distance $\leftarrow$ 0\;
	
	\Pour{pos $ \in 1 \ldots |orig|$ }{
		\lSi{orig[pos] $\ne$ modif[pos]}{
			distance $\leftarrow$ distance + 1
		}
	}

	\caption{Calcul de la distance de Hamming \label{algo:hamming}}
	
\end{algorithm}


\subsubsection{Plus longue sous-séquence commune}

Cette distance permet l'insertion et la suppression.
Parmi ses utilisations, la détection des modifications dans le programme ``diff" (utilisé aussi dans ``Git").
En anglais, elle s'appelle ``Longest common subsequence" (LCS).
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous définissons un tableau $C[m, n]$ à deux dimensions contenant la longueur de la LCS.
L'équation \ref{eq:LCS-length} peut être utilisée afin de calculer la longueur de la plus longue sous-séquence. 
\begin{equation}
	C[i, j] =  
	\begin{cases}
		0 & \text{Si } i = 0 \text{ ou } j=0\\
		C[i-1, j-1] + 1 & \text{Si } i,j > 0 \text{ et } x_i = y_j\\
		max (C[i, j-1], C[i-1, j]) & \text{Si } i,j > 0 \text{ et } x_i \ne y_j\\
	\end{cases}
	\label{eq:LCS-length}
\end{equation}

Dans ce cas, la longueur de la plus longue sous-séquence $|LCS(X, Y)| = C[m, n]$.
La distance sera calculée en utilisant l'équation \ref{eq:LCS-distance}.
\begin{equation}
	D(X, Y) = m + n - 2 |LCS(X, Y)|
	\label{eq:LCS-distance}
\end{equation}

\subsubsection{Distance de Levenshtein}

Cette distance permet l'insertion, la suppression et la substitution.
En général, elle est utilisée dans la recherche approximative et la vérification d'orthographe.
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous définissons un tableau $D[m, n]$ à deux dimensions contenant la distance d'édition entre les sous-chaînes X[1..i] et Y[1..j]. 
Dans ce cas $D[0, 0] = 0$ et la distance finale sera stockée dans $D[m, n]$.
L'équation \ref{eq:lev-distance} formule comment on calcule cette distance (en utilisant la programmation dynamique).
Il faut savoir que dans cette version, le coût de la suppression et l'insertion est $1$ et le coût de la substitution est $2$.
\begin{equation}
	D[i, j] = \min 
	\begin{cases}
		D[i - 1, j] + 1 \text{ //Suppression}\\
		D[i, j-1] + 1 \text{ //Insertion}\\
		D[i-1, j-1] + \begin{cases}
			2 & \text{si } x_i \ne y_j \\
			0 & \text{sinon}
		\end{cases}
	\end{cases}
	\label{eq:lev-distance}
\end{equation}

Par exemple, \expword{D(amibe, immature) = 9}. 
Dans ce cas, ``a" et ``m" sont supprimés (1+1) ; ``i" reste le même (0) ; les caractères ``m", ``m", ``a", ``t" et ``u"  sont insérés (1+1+1+1+1), ``b" est substitué par ``r" (2) et ``e" reste le même (0).
Le calcul est illustré dans la figure \ref{fig:laven-distance} où les flèches représentent d'où vient la valeur et les cellules colorées représentent le chemin choisi.
Bien sûr, nous pouvons choisir un autre chemin qui donne une autre interprétation.
Pour aider le choix du chemin, nous pouvons utiliser des probabilités des opérations d'édition.
Par exemple, dans la correction d'orthographe, des lettres sont plus probables d'être remplacées par d'autres (celles adjacentes dans le clavier).
\begin{figure}[ht]
	\centering
	\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|}
		\hline
		&\bfseries \# &\bfseries i &\bfseries m &\bfseries m &\bfseries a &\bfseries t &\bfseries u &\bfseries r &\bfseries e \\
		\hline
		\bfseries \# & 0 & $ \leftarrow $ 1 & $ \leftarrow $ 2 & $ \leftarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 & $ \leftarrow $ 8\\
		\hline
		\bfseries a & \cellcolor{green!25} $ \uparrow $ 1 & $ \nwarrow\leftarrow\uparrow $ 2 & $ \nwarrow\leftarrow\uparrow $ 3 & $ \nwarrow\leftarrow\uparrow $ 4 & $ \nwarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 \\
		\hline
		\bfseries m & \cellcolor{green!25} $ \uparrow $ 2 & $ \nwarrow\leftarrow\uparrow $ 3 & $\nwarrow $ 2 & $\nwarrow\leftarrow $ 3 & $\leftarrow\uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8\\
		\hline
		\bfseries i & $ \uparrow $ 3 & \cellcolor{green!25} $ \nwarrow $ 2 & \cellcolor{green!25} $\leftarrow\uparrow $ 3 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 4 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 5 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 6 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & $\nwarrow\leftarrow\uparrow $ 9\\
		\hline
		\bfseries b & $ \uparrow $ 4 & $ \uparrow $ 3 & $\nwarrow\leftarrow\uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & \cellcolor{green!25} $\nwarrow\leftarrow\uparrow $ 9 & $\nwarrow\leftarrow\uparrow $ 10\\
		\hline
		\bfseries e & $ \uparrow $ 5 & $ \uparrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow\leftarrow\uparrow $ 6 & $\nwarrow\leftarrow\uparrow $ 7 & $\nwarrow\leftarrow\uparrow $ 8 & $\nwarrow\leftarrow\uparrow $ 9 & $\nwarrow\leftarrow\uparrow $ 10 & \cellcolor{green!25} $\nwarrow $ 9\\
		\hline
	\end{tabular}
	\caption[Exemple de calcul de distance de Levenshtein.]{Exemple de calcul de distance de Levenshtein des mots ``amibe"  et ``immature" ; figure inspirée de \cite{2019-jurafsky-martin}.}
	\label{fig:laven-distance}
\end{figure}
%\begin{figure}[ht]
%	\centering
%	\hgraphpage[.75\textwidth]{exp-levenshtein_.pdf}
%	\caption[Exemple de calcul de distance de Levenshtein]{Exemple de calcul de distance de Levenshtein \cite{2019-jurafsky-martin} \label{fig:laven-distance}}
%\end{figure}

\subsubsection{Distance de Damerau–Levenshtein}

Cette distance permet l'insertion, la suppression, la substitution et la transposition entre deux caractères adjacents.
En général, elle est utilisée dans la vérification d'orthographe.
Cette distance est calculée comme celle de Levenstein, en ajoutant la transposition entre deux caractères adjacents.
L'équation \ref{eq:lev-dem-distance} représente comment calculer cette distance.
Dans cette version, nous avons essayé d'attribuer le poids $1$ pour toutes les opérations d'édition.
\begin{equation}
D[i, j] = \min 
	\begin{cases}
		D[i - 1, j] + 1 \text{ //Suppression}\\
		D[i, j-1] + 1 \text{ //Insertion}\\
		D[i-1, j-1] + \begin{cases}
			1 & \text{si } x_i \ne y_j \text{ //Substitution}\\
			0 & \text{sinon}
		\end{cases}\\
		D[i-2, j-2] + 1 \text{ si } x_i = y_{j-1} \text{ et } x_{i-1} = y_j \text{ //Transposition}\\
	\end{cases}
	\label{eq:lev-dem-distance}
\end{equation}

Par exemple, \expword{D(amibe, immature) = 6}. 
Dans ce cas, ``i" et ``m" sont insérés (1+1) ; ``a" est transposé avec ``m" (1) ; ``t" est inséré (1) ; ``i" est substitué par ``u" (1) ;  ``b" est substitué par ``r" (1) et ``e" reste le même (0).
Le calcul est illustré dans la figure \ref{fig:dam-laven-distance} où les flèches représentent d'où vient la valeur et les cellules colorées représentent le chemin choisi.
Lorsqu'il y a une transposition, nous sautons deux cellules diagonales.
\begin{figure}[ht]
	\centering
	\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|}
		\hline
		&\bfseries \# &\bfseries i &\bfseries m &\bfseries m &\bfseries a &\bfseries t &\bfseries u &\bfseries r &\bfseries e \\
		\hline
		\bfseries \# & 0 & \cellcolor{green!25} $ \leftarrow $ 1 & \cellcolor{green!25} $ \leftarrow $ 2 & $ \leftarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 & $ \leftarrow $ 8\\
		\hline
		\bfseries a & $ \uparrow $ 1 & $ \nwarrow $ 1 & $ \nwarrow\leftarrow $ 2 & $ \nwarrow\leftarrow $ 3 & $ \nwarrow $ 3 & $ \leftarrow $ 4 & $ \leftarrow $ 5 & $ \leftarrow $ 6 & $ \leftarrow $ 7 \\
		\hline
		\bfseries m & $ \uparrow $ 2 & $ \nwarrow\uparrow $ 2 & $\nwarrow $ 1 & $\nwarrow\leftarrow $ 2 & \cellcolor{green!25} $\leftrightharpoons\leftarrow $ 3 & \cellcolor{green!25} $\leftarrow $ 4 & $\leftarrow $ 5 & $\leftarrow $ 6 & $\leftarrow $ 7\\
		\hline
		\bfseries i & $ \uparrow $ 3 & $ \nwarrow $ 2 & $\leftrightharpoons\uparrow $ 2 & $\nwarrow\leftarrow\uparrow $ 3 & $\nwarrow $ 3 &  $\nwarrow\leftarrow $ 4 & \cellcolor{green!25} $\nwarrow\leftarrow $ 5 & $\nwarrow\leftarrow $ 6 & $\nwarrow\leftarrow $ 7\\
		\hline
		\bfseries b & $ \uparrow $ 4 & $ \uparrow $ 3 & $\nwarrow\uparrow $ 3 & $\nwarrow $ 3 & $\nwarrow\leftarrow\uparrow $ 4 & $\nwarrow $ 4 & $\nwarrow\leftarrow $ 5 & \cellcolor{green!25} $\nwarrow\leftarrow $ 6 & $\nwarrow\leftarrow $ 7\\
		\hline
		\bfseries e & $ \uparrow $ 5 & $ \uparrow $ 4 & $\nwarrow\uparrow $ 4 & $\nwarrow\uparrow $ 5 & $\nwarrow $ 4 & $\nwarrow\leftarrow\uparrow $ 5 & $\nwarrow $ 5 & $\nwarrow\leftarrow $ 6 & \cellcolor{green!25} $\nwarrow $ 6\\
		\hline
	\end{tabular}
	\caption[Exemple de calcul de distance de Damerau–Levenshtein]{Exemple de calcul de distance de Damerau–Levenshtein des mots ``amibe"  et ``immature".}
	\label{fig:dam-laven-distance}
\end{figure}

\subsubsection{Distance de Jaro}

Cette distance permet seulement la transposition.
En réalité c'est une mesure de similarité qui retourne une valeur entre $0$ (pas de similarité) et $1$ (similaires). 
Elle est utilisée pour le calcul de la similarité entre les entités nommées, etc.
Étant donné deux chaînes $X$ et $Y$ avec les longueurs $n$ et $m$ respectivement, nous calculons le nombre des caractères correspondants $c$ et le nombre des transpositions $t$. 
La distance de Jaro est calculée selon l'équation \ref{eq:jaro-distance}.
\begin{equation}
	D(X, Y) = 
	\begin{cases}
	0 & \text{si } c = 0\\
	\frac{1}{3} (\frac{c}{m} + \frac{c}{n} + \frac{c-t}{c}) & \text{sinon}
	\end{cases}
	\label{eq:jaro-distance}
\end{equation}

Le nombre des caractères correspondants $c$  est le nombre des caractères identiques de $X$ et de $Y$ avec un éloignement $ e= \max (m, n)/2 - 1$. Dans ce cas, si nous soyons dans le caractère $i$ (allant de $0$ jusqu'à $n-1$) du mot $X$, nous commencerions la comparaison par le caractère $j=\max(0, i - e)$ du mot $Y$ et nous terminerions par le caractère $j=\min(i+e, m-1)$.
Dans cette opération, nous pouvons préparer deux vecteurs des booléens qui indiquent la position du caractère ayant un correspondant dans l'autre mot.
%
Le nombre des transpositions $t$ est calculé en comparant le i\textsuperscript{ème} caractère correspondant de $X$ avec le i\textsuperscript{ème} caractère correspondant de $Y$. 
Nous comptons le nombre des fois $x_i \ne y_i$  et nous le divisons par deux (voir l'algorithme \ref{algo:jaro-transpo}).
\vspace{-6pt}
\begin{algorithm}[ht]
	\KwData{Xmatch, Ymatch : vecteurs de booléens}
	\KwResult{t: entier}
	t $\leftarrow$ 0 ; j $\leftarrow$ 0\;
	
	\Pour{i $ \in 0 \ldots m$ }{
		\Si{Xmatch[i] = Vrai}{
			\lTq{Ymatch[j] $\ne$ Vrai}{
				j $\leftarrow$ j + 1
			}
			
		    \lSi{$x_i \ne y_j$}{
		    	t $\leftarrow$ t + 1
		    }
	    	j $\leftarrow$ j + 1\;
		}
	}
    
    t $\leftarrow$ t/2\;
    
    \caption{Calcul du nombre de transpositions entre deux mots X et Y dans la distance de Jaro \label{algo:jaro-transpo}}
	
\end{algorithm}

Prenons l'exemple de ``amibe"  et ``immature" ayant les tailles 5 et 8 respectivement.
L'éloignement sera $e=\max(5, 8)/2 - 1 = 3$.
La figure \ref{fig:jaro} représente le calcul de la distance entre ces deux mots en utilisant les deux ordres possibles.
Dans la matrice, $0$ veut dire ``Faux" et $1$  veut dire ``Vrai" (correspondance). 
Les cases en gras représentent la plage de recherche en utilisant l'éloignement de 3. 
Les cases soulignées représentent l'intersection entre la ième correspondance de X et la ième correspondance de Y.
Les correspondances soulignées des vecteurs des deux mots représentent des transpositions. 

\begin{figure}[ht]
	
\begin{minipage}{0.45\textwidth}
\small
\begin{tabular}{|l|l|l|l|l|l|l|l|}
	\cline{1-6}
	  & a & m & i & b & e & \multicolumn{2}{l}{ }\\
	\cline{1-6}\cline{8-8}
	i & \underline{\textbf{0}} & \textbf{0} & \textbf{1} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	m & \textbf{0} & \underline{\textbf{1}} & \textbf{0} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	m & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	a & \textbf{1} & \textbf{0} & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & & 1\\
	\cline{1-6}\cline{8-8}
	t & 0 & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	u & 0 & 0 & \textbf{0} & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	r & 0 & 0 & 0 & \textbf{0} & \textbf{0} & & 0\\
	\cline{1-6}\cline{8-8}
	e & 0 & 0 & 0 & 0 & \underline{\textbf{1}} & & 1\\
	\cline{1-6}\cline{8-8}
	\multicolumn{8}{l}{ }\\
	\cline{2-6}
	 \multicolumn{1}{l|}{} & \underline{1} & 1 & \underline{1} & 0 & 1 & \multicolumn{2}{l}{ }\\
	\cline{2-6}
\end{tabular}

\end{minipage}
\begin{minipage}{0.45\textwidth}

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
	\cline{1-9}
	  & i & m & m & a & t & u & r & e & \multicolumn{2}{l}{ }\\
	\cline{1-9}\cline{11-11}
	a & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & \textbf{1} & 0 & 0 & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	m & \textbf{0} & \underline{\textbf{1}} & \textbf{0} & \textbf{0} & \textbf{0} & 0 & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	i & \textbf{1} & \textbf{0} & \textbf{0} & \underline{\textbf{0}} & \textbf{0} & \textbf{0} & 0 & 0 & & 1\\
	\cline{1-9}\cline{11-11}
	b & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & 0 & & 0\\
	\cline{1-9}\cline{11-11}
	e & 0 & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \underline{\textbf{1}} & & 1\\
	\cline{1-9}\cline{11-11}
	\multicolumn{11}{l}{ }\\
	\cline{2-9}
	\multicolumn{1}{l|}{} & \underline{1} & 1 & 0 & \underline{1} & 0 & 0 & 0 & 1 & \multicolumn{2}{l}{ }\\
	\cline{2-9}
\end{tabular}
\end{minipage}
\caption[Exemple de calcul de la distance de Jaro.]{Exemple de calcul de la distance de Jaro des mots ``amibe"  et ``immature".}
\label{fig:jaro}
\end{figure}

%===================================================================================
\section{Segmentation du texte}
%===================================================================================

Un texte peut être traité lorsqu'il est segmenté en passages de petites tailles ; ils peuvent être des chapitres, des paragraphes, des phrases ou des mots selon la granularité voulue.
Dans plusieurs langues, les phrases sont délimitées par un point ou une marque spécifique, et les mots sont délimités par des espaces. 
Même dans ces langues, le délimiteur peut être utilisé pour d'autres fins. 
Il existe des langues où il n'y a pas de délimiteur des phrases, des mots ou des deux.

\subsection{Délimitation de la phrase}

Afin de séparer les phrases dans un format semi-structuré, comme HTML, nous pouvons utiliser des marqueurs comme la balise \expword{\textless P\textgreater}.
Mais, lorsqu'il s'agit d'un texte non structuré, plusieurs langues utilisent des marqueurs comme le point, point d'exclamation et point d'interrogation pour marquer la fin d'une phrase. 
Dans ce cas, nous pouvons utiliser une expression régulière simple \expword{/[.?!]/} pour délimiter les phrases dans des langues comme français, anglais, etc.
Des fois, nous voulons séparer les phrases longues avec des clauses séparées par des virgules.
Aussi, les guillemets peuvent causer un problème : est-ce que la phrase dedans est séparée ou elle est une continuation de la clause principale ?
Par exemple, \expword{Il a dit : ``Je suis fatigué." en retournant à son lit.}.
Le point dans ces langues n'est pas toujours utilisé pour séparer les phrases. 
Il peut être utilisé dans les nombres : \expword{123,456.78 (style américain) 123.456,78 (style européen)}.
Les abréviations comme ``Mr.", ``Dr." et ``etc." contiennent des points et peuvent se trouver au milieu de la phrase. 
Ils peuvent, aussi, se trouver à la fin ; donc, il faut vraiment être capable de détecter les abréviations et s'ils marquent la fin de la phrase ou non. 
Si tous ça ne parait pas problématique, nous pouvons toujours essayer de séparer les phrase du thaï. 
Cette langue n'utilise pas des marqueurs pour séparer les phrases.

Lorsque le marqueur de phrase est réservé pour d'autres utilisations, il y a toujours des facteurs qui aident le lecteur à décider si c'est un délimiteur de phrase ou non.
Ces facteurs peuvent être extraits en observant comment la langage définit la fin de la phrase. 
Quelques facteurs contextuels ont été proposés et utilisés pour la segmentation des phrases \cite{10-palmer} :
\begin{itemize}
	\item \optword{La casse} : les phrases commencent toujours par un majuscule. 
	Mais, ce n'est pas toujours le cas ; nous pouvons trouver une abréviation suivie par un nom propre (Ex. ``\expword{M. Aries}"). 
	En plus, ce n'est pas garanti que l'écrivain respecte les règles d'écriture. 
	Nous pouvons voir cela, par exemple, dans les réseaux sociaux où plusieurs règles sont abandonnées.
	
	\item \optword{Noms propres} : les noms propres commencent par un majuscule ; ils peuvent ne pas être le début.
	Dans l'exemple précédent (``\expword{M. Aries}"), nous pouvons déduire que le point ne représente pas une séparation vu que les deux mots commencent par un majuscule et le deuxième est un nom propre. 
	Dans ce cas, la probabilité que le premier soit une abréviation est grande surtout si le mot est au milieu de la phrase.
	
	\item \optword{Catégorie grammaticale} : les catégories des mots qui entourent le point peuvent aider la décision (limite ou non). 
	En fait,  \citet{97-palmer-hearst} ont été capables d'améliorer la détection des limites des phrases en utilisant les catégories grammaticales des deux mots avant et après le point avec un algorithme d'apprentissage automatique. 
	
	\item \optword{Longueur du mot} : les abréviations sont moins longues.
	Revenons toujours à l'exemple précédent, il est clair que ``M" n'est pas vraiment un mot. 
	
	\item \optword{Préfixes et suffixes} : les mots avec des affixes sont moins probables d'être des abréviations.
	
	\item \optword{Classes des abréviations} : les abréviations peuvent figurer à la fin de la phrase. 
	Mais, il y a un ensemble d'abréviations qui sont toujours suivies par un autre mot, comme par exemple ``Mr.".
	\citet{89-riley,97-reynar-ratnaparkhi} divisent les abréviations en deux catégories : les titres (qui ne peuvent pas être à la fin de phrase (Ex. ``\expword{Mr.}", ``\expword{Dr.}", etc.) et les indicatifs corporatifs (qui peuvent être à la fin de phrase (Ex. ``\expword{Corp.}", ``\expword{S.P.A.}", etc.).
\end{itemize}

%Les différents algorithmes de détection de limites
La détection automatique des limites d'une phrase peut être accomplie en utilisant des règles manuelles ou en utilisant l'apprentissage automatique. 
Dans la première approche, nous pouvons utiliser des expressions régulières pour détecter les délimiteurs. 
Ensuite, nous pouvons utiliser une liste des abréviations pour améliorer la décision. 
Les règles peuvent être enrichies en introduisant les facteurs discutés précédemment. 
Afin d'éviter l'écriture manuelle de ces règles, nous pouvons utiliser un algorithme d'apprentissage automatique qui classe le point comme délimiteur/non-délimiteur en se basant sur ces même règles.


\subsection{Séparation des mots}

Plusieurs langues (arabe, français, anglais, etc.) utilisent l'espace comme délimiteur des mots.
Une expression régulière simple comme \expword{/[ ]+/} peut être utilisée pour séparer les mots. 
Mais, des fois nous voulons récupérer une expression avec plusieurs mots ; comme le cas des dates, des nombres, etc. 
Un exemple des nombres est ``\expword{neuf cent quarante}" ; cette expression peut être considérée comme un seul token dans certains traitements.
Dans des langues, l'apostrophe peut être une source d'ambigüité. 
Dans l'anglais, l'apostrophe peut être utilisée avec un ``\textit{s}" dans la forme possessive (\expword{Karim's thesis}), dans les contractions (\expword{she's, it's, I'm, we've}) ou dans le pluriel de certains mots (\expword{I.D.'s, 1980's}). 
Dans le français, il y a pas mal d'exemples de contractions : la contraction des articles (\expword{l'homme, c'était}), la contraction des pronoms (\expword{j'ai, je l'ai}), et autres formes (\expword{n'y, qu'ils, d'ailleurs}).
Certaines langues utilisent des mots composés, soit par composition ou par trait d'union. 
Dans l'allemand, il est commun d'utiliser la composition des mots: nom-nom (\expword{Lebensversicherung: assurance vie}), adverbe-nom (\expword{Nichtraucher: non-fumeur}), et préposition-nom (\expword{Nachkriegszeit: période d'après-guerre}). 
D'autres langues utilisent le trait d'union, comme l'anglais (\expword{end-of-file, classification-based}) et le français (\expword{va-t-il, c'est-à-dire, celui-ci}). 
Il existe des langues, comme le japonais, qui n'utilisent pas de marqueurs pour séparer les mots (\expword{今年は本当に忙しかったです。}).


Il existe deux approches pour la séparation des mots : par règles et statistique. 
L'approche par règles utilise principalement les expressions régulières en se basant sur les règles morphologiques.
Elle peut aussi utiliser des listes des mots. 
Par exemple, dans une langue de type \keyword{Scriptio Continua} comme le japonais et le chinois, nous pouvons utiliser un dictionnaire et comparer les mots en commençant par la fin en prenant la séquence la plus longue.
L'approche statistique utilise un modèle de langage pour calculer la probabilité qu'un caractère marque la fin d'un mot. 
Les modèles de langages seront présentés dans le chapitre suivant. 

%Approches
%\begin{itemize}
%	\item Par règles : en utilisant des expressions régulières 
%	\begin{itemize}
%		\item \url{https://www.nltk.org/api/nltk.tokenize.html}
%		\item \url{https://nlp.stanford.edu/software/tokenizer.shtml}
%		\item \url{https://spacy.io/}
%		\item \url{https://github.com/kariminf/jslingua}
%		\item \url{https://github.com/linuxscout/pyarabic}
%	\end{itemize}
%	\item Statistique : en utilisant un modèle de langue pour calculer la probabilité qu'un caractère marque la  fin d'un mot 
%	\begin{itemize}
%		\item \url{https://nlp.stanford.edu/software/segmenter.html}
%		\item \url{https://opennlp.apache.org/}
%	\end{itemize}
%\end{itemize}

%===================================================================================
\section{Normalisation du texte}
%===================================================================================

Un texte peut contenir des variations du même terme. 
Dans des tâches qui se basent sur les statistiques sur des mots (comme la recherche d'information), nous devons traiter ces variations comme un seul token. 
Même dans des tâches comme la compréhension de la langue, nous avons besoin de trouver des formes standards des dialectes (comme ``\expword{ain't}").
La transformation du texte à une forme canonique, en général, se fait en utilisant des expressions régulières pour chercher les variations et un dictionnaire pour chercher leurs formes canoniques. 
Parmi les variations qui ont besoin d'être normalisées, nous pouvons citer :
\begin{itemize}
	\item \optword{Acronymes et les abréviations} : des fois, nous trouvons plusieurs variations d'une même abréviation.
	Dans ce cas, il faut choisir une seule variation pour les représenter. 
	Par exemple, \expword{US \textrightarrow\ USA, U.S.A. \textrightarrow\ USA}.
	Dans les tâches où nous avons besoin d'une compréhension plus approfondie, nous devons chercher la version longue. 
	Par exemple, \expword{M. \textrightarrow\ Monsieur}.
	
	\item \optword{Valeurs numériques} (dates et nombres) : 
	selon la tâche, nous devons unifier le format des valeurs numériques. 
	Des fois, nous avons besoin de trouver la forme textuelle (\expword{1205 DZD \textrightarrow\ Mille deux cents cinq dinars algériens}). 
	Cela est utile, par exemple, dans le cas de la synthèse de parole où l'appareil doit prononcer ce qui est écrit.
	Lorsque nous avons plusieurs sources de textes, nous tombons sur plusieurs variations des dates. 
	Dans ce cas, nous pouvons utiliser le standard \keyword{ISO 8601} (\expword{12 Janvier 1986, 12.01.86 \textrightarrow\ 1986-01-12}).
	Dans les tâches qui s'intéressent aux statistiques (Ex. nombre des dates dans un texte), nous n'avons pas besoin de garder les formes numériques. 
	Dans ce cas, nous pouvons garder seulement le type de ces formes (\expword{12 Janvier 1986 \textrightarrow\ DATE, kariminfo0@gmail.com \textrightarrow\ EMAIL}).
	
	\item \optword{Majuscules et Minuscules} : dans la plupart des cas, nous n'avons pas besoin de garder les mots en majuscules (\expword{Texte \textrightarrow\ texte}). 
	Mais, il faut faire attention lorsque nous avons besoin de garder cette information pour des tâches comme la segmentation du texte, ou pour différencier les nom propres (\expword{Will}).
	
	\item \optword{Diacritiques} : comme le point précédent, les diacritiques peuvent être supprimées lorsque la tâche ne dépend pas sur elles. 
	Par exemple, dans le français, nous appelons cela : désaccentuation (\expword{système \textrightarrow\ systeme}).
	Dans l'arable, nous l'appelons : dé-vocalisation (\expword{\<yadrusu> \textrightarrow\ \<ydrs>}). 
	Dans des tâches qui ont besoin de vocalisation, comme le traitement des poèmes, nous devons garder les diacritiques. 
	En fait, nous devons appliquer l'opération inverse (vocalisation du texte).
	
	\item \optword{Contractions} : en général, nous trouvons ces formes beaucoup plus sur les réseaux sociaux. 
	Mais, elles peuvent aussi être une règle standard de la langue. 
	Par exemple, \expword{y'll \textrightarrow\ you all, s'il \textrightarrow\ si il}. 
	Dans le cas du dernier exemple, ça va aider dans la tâche de séparation des mots (tokenization). 
	
	\item \optword{Encodage} : il faut utiliser le même encodage supporté dans le traitement. 
	L'encodage le plus célèbre est \keyword[U]{UTF-8}.

\end{itemize}


%===================================================================================
\section{Filtrage du texte}
%===================================================================================

Le texte peut contenir des caractères, des mots et des expressions qui peuvent entraver son traitement. 
Pour faciliter ce dernier, il faut supprimer le bruit. 
Un exemple très commun est la présence des caractères spéciaux, comme les caractères non imprimables, dans le texte. 
Les mots contenant ces caractères ne sont pas considérés les mêmes que les mots sans ces caractères. 
En général, ces caractères sont d'origine des pages web ou des PDFs (extraction du texte à partir des PDFs ou autres formes d'images).
Les mots clés des formats textuelles est un exemple des mots à filtrer. 
Nous pouvons trouver ça dans les balises de certaines formats semi-structurées comme HTML, XML, etc.


Les \optword{mots vides} représentent les mots non significatifs comme les prépositions, articles et les pronoms.
La suppression des mots vides peut être utilisée si plusieurs mots dans le document ne contribuent pas particulièrement dans la description de son contenu.
Afin d'augmenter la performance du système, plusieurs tâches (comme la recherche d'information et de résumé automatique) font appel à cette technique.
Mais, nous pouvons tomber sur des cas particuliers où cela peut causer des problèmes. 
Par exemple, la phrase ``\expword{To be or not to be}" sera totalement ignorée vu que tous ses mots sont des mots vides.
Le nom propre ``\expword{Will}" peut être ignoré par confusion avec le verbe ``to be" en future.


%===================================================================================
\section{Morphologie}
%===================================================================================

Nous avons vu dans le chapitre précédent qu'il existe pas mal de langues qui permettent la formation des mots en utilisant la flexion (ex. \expword{conjugaison}) et la dérivation (ex. \expword{nominalisation}). 
La formation des mots la plus utilisée est l'affixation en suivant certains règles. 
Par exemple, pour conjuguer le verbe ``\expword{étudier}" avec le pronom ``\expword{nous}" en présent, nous supprimons le suffixe ``\expword{er}" pour avoir le radical ``\expword{étudu}" et nous ajoutons le suffixe ``\expword{ons}". 
L'automatisation de cette tâche peut aider plusieurs applications, comme la génération du langage naturel (anglais : NLG). 
La tâche inverse consiste à trouver une forme standard des différentes variations ; c'est une technique de normalisation.
Elle peut aider dans des tâches comme la recherche d'information et la compréhension du langage naturel (anglais : NLU).

\subsection{Formation des mots}

Dans les langues synthétiques, nous pouvons former les mots en utilisant la flexion ou la dérivation. 
La flexion génère des variations morphologiques d'un mot selon les traits grammaticaux (nombre, genre, etc.). 
Les deux types de flexion sont la conjugaison des verbes et la déclinaison des noms, pronoms, adjectifs et déterminants.
Quant à la dérivation, les mots créés forment un nouveau lexèmes (un sens différent du mot original) ou ils appartiennent à une autre catégorie grammaticale. 
Un exemple d'un nouveau lexème,  \expword{couper \textrightarrow\ découper, \<`ml> \textrightarrow\ \<ist`ml>}.
Le changement de catégorie peut être dû à la nominalisation (\expword{classer \textrightarrow\ classement, classeur ; \<darasa> \textrightarrow\ \<darsuN, madrasaTuN, mudarrisuN, dArisuN>}) ou l'adjectif (\expword{fatiguer \textrightarrow\ fatigant}), etc.


La formation des mots suit des règles bien définies, donc le problème peut être résolu en utilisant un automate à état fini. 
Bien sûr, il existe des cas spéciaux qui peuvent simplement être stockés dans un fichier. 
L'approche par règles est beaucoup plus utilisée dans ce cas, mais nous pouvons trouver des recherches qui utilisent le niveau caractère pour appendre à générer des formes d'un mot donné, comme le projet MLConjug\footnote{MLConjug : \url{https://github.com/SekouD/mlconjug} [visité le 2021-09-08]}. 
D'un point de vu, l'apprentissage automatique doit être utilisé pour résoudre les problèmes vraiment difficiles (en général, niveau syntaxique, sémantique, et pragmatique). 
Autre que l'approche statistique, plusieurs méthodes traditionnelles ont été utilisées pour la formation des mots. 
Dans le contexte du conjugaison automatique des verbes, nous pouvons citer :
\begin{itemize}
	\item \optword{Base de données} : dans cette solution, nous stockons tous les verbes dans une base de donnée avec les différentes variations possibles. 
	Le point fort de cette solution est que nous puissions vérifier si un verbe appartient à la langue. 
	Aussi, en arabe, les verbes peuvent avoir les mêmes lettres ; la seule différence est en vocalisation (diacritiques).
	Malgré ces points forts, cette solution est vraiment difficile à être implémentée ; nous devons chercher tous les verbes et toutes les variations possibles. 
	
	\item \optword{Modèles (\textit{template})} : dans cette solution, nous stockons les conjugaisons de certains verbes comme modèles et une autre liste de tous les verbes de la langue avec leurs modèles respectifs.
	C'est la forme la plus utilisée en français (par exemple, le verbe ``\expword{sourire} a comme modèle le verbe ``\expword{rire}"). 
	Elle est similaire à la solution précédente, mais avec moins d'espace de stockage.
	
	\item \optword{Règles} : dans cette solution, nous utilisons des règles SI-SINON et des expressions régulières.
	Un exemple de cette méthode peut être trouvé dans le projet Qutrub\footnote{Qutrub : \url{https://github.com/linuxscout/qutrub} [visité le 2021-09-08]} pour la conjugaison automatique de l'arabe.  
	Un autre exemple de ce type est celui de JsLingua\footnote{JsLingua : \url{https://github.com/kariminf/jslingua} [visité le 2021-09-08]} pour la conjugaison des verbes en arabe, anglais, français et japonais. 
	L'avantage est que nous n'avons pas besoin de créer une base de données ; la solution est plus rapide. 
	Mais, nous devons gérer beaucoup de règles et aussi nous aurions besoin d'une base de données si nous voulions vérifier le type du verbe (il peut y avoir des verbes confondus).

\end{itemize}

\subsection{Réduction des formes}

Il y a deux types réduits d'une forme d'un mot : radical et lemme. 
Le radical est un morphème qui peut ne pas être un mot du vocabulaire, or le lemme est un mot qui représente le lexème.
La radicalisation (racinisation, en anglais : stemming) est l'opération de supprimer les affixes afin d'obtenir un radical (racine, en anglais : stem). 
Par exemple, \expword{chercher \textrightarrow\ cherch}. 
Cette tâche est rapide et préférée dans des tâches comme la recherche d'information où nous avons besoin de plus de vitesse d'exécution même au détriment de l'exactitude. 
Quelques techniques pour la radicalisation sont les suivantes :
\begin{itemize}
	\item \optword{Base de données} : Stocker tous les termes et leurs racines dans une table. 
	\item \optword{Statistiques} : Utiliser un modèle de langage (comme les \keywordpl[N]{N-Gramme}) pour estimer la position de troncation.
	\item \optword{Règles} : Utiliser un ensemble de règles condition/action afin de détecter les affixes et les tronquer. 
	L'algorithme le plus connu est celui de Porter \cite{1980-porter} pour l'anglais. 
	Il existe un framework très connu, appelé SnowBall\footnote{SnowBall : \url{https://snowballstem.org/} [visité le 2021-09-08]}, pour réaliser des racinateurs de ce genre. 
	Dans ce framework, plusieurs conditions sont utilisées : sur la racine, sur l'affixe ou sur la règle. 
	Une condition sur la racine peut être la longueur, la fin, si elle contient des voyelles, etc.
	Exemple, \expword{(*v*) Y \textrightarrow\ I : happy \textrightarrow\ happi, sky \textrightarrow\ sky}.
	Une condition sur l'affixe peut être ``il n'y a que le suffixe".
	Exemple, \expword{SSES \textrightarrow\ SS, ATIONAL \textrightarrow\ ATE}.
	Une condition sur la règle peut être : la désactivation de certaines règles si une a été exécutée.
\end{itemize}

La lemmatisation (en anglais : lemmatization) cherche la forme canonique d'un mot appelée ``lemme" (en anglais : lemma).
Exemple, \expword{comprennent \textrightarrow\ comprendre, better \textrightarrow\ good}
Cette tâche est plus difficile que celle de radicalisation, puisque nous avons besoin du contexte du mot (Ex. \expword{saw \textrightarrow\ (V) see ou (N) saw}). 
\begin{itemize}
	\item \optword{Bases lexicales} : Ici, nous utilisons la radicalisation avec d'autres règles afin de chercher une forme dans une liste des lemmes possibles (un dictionnaire).
	Un exemple de ce type de lemmatisation est la lemmatisation morphy de Wordnet (voir l'algorithme \ref{algo:morphy}).
	
	\item \optword{Apprentissage automatique} : Nous essayons d'apprendre le lemme des mots en se basant sur des critères comme leurs catégories grammaticales.
	L'outil OpenNLP\footnote{OpenNLP lemmatisation : \url{https://opennlp.apache.org/docs/1.8.0/manual/opennlp.html\#tools.lemmatizer} [visité le 2021-09-08]} implémente une version statistique de lemmatisation.

\end{itemize}

\begin{algorithm}[H]
		\KwData{mot, catégorie}
		\KwResult{liste des lemmes possibles}
		
		\Si{mot $ \in $  list\_exceptions[catégorie]}{
			\Return chercher\_dans\_dictionnaire(\{mot\} $ \cup $ list\_exceptions[catégorie][mot])\;
		}
		
		formes = \{mot\}
		
		\Tq{formes $ \ne \emptyset $}{
			formes = supprimer\_les\_affixes(formes, catégorie)\;
			
			résultats = chercher\_dans\_dictionnaire(\{mot\} $ \cup $ formes)\;
			
			\Si{résultats $ \ne \emptyset $ }{
				\Return résultats \;
			}
		}
		
		\Return $ \emptyset $\;
		
		\caption{Lemmatisation "morphy" de Wordnet \label{algo:morphy}}
		
	\end{algorithm}

\sectioni{Discussion}
%\begin{discussion}
	La morphologie est le niveau le plus simple dans le traitement d'un langage. 
	Les tâches dans ce niveau sont beaucoup plus basées sur le caractère (graphème) qui est l'unité la plus basique dans le système d'écriture.  
	La plupart des problèmes peuvent être résolus avec un automate à état fini ; d'où l'utilisation des expressions régulières. 
	Ces dernières peuvent être utilisées pour rechercher des mots, extraire les phrases et les mots, extraire des parties du mot (racine), etc. 
	Toutes ces tâches reviennent à la recherche d'un segment dans le texte. 
	Pour une recherche approximative, des méthodes de comparaison dans le niveau caractère (distance d'édition) sont utilisées. 
	
	Le texte se compose des unités qui peuvent être des chapitres, des phrases, des mots ou des caractères.
	Le mot est l'unité la plus petite ayant un sens. 
	Donc, afin de traiter un texte, il faut le décomposer en petites unités  afin de traiter les unités plus grandes, etc. 
	D'où l'importance de la segmentation du texte. 
	Ces unités peuvent avoir le même sens, mais plusieurs variations. 
	Dans la compréhension du texte, nous essayons de représenter le texte d'une manière plus abstraite.
	Donc, une variation peut être représentée par un mot représentant plus des caractéristiques comme le genre, le nombre, etc. 
	Quelques mots doivent être filtrés puisqu'ils sont considérés comme du bruit.
	
	Certes, les tâches de ce niveau peuvent être simples. 
	Mais, elles sont vraiment primordiales pour la réussite des tâches plus évoluées. 
	Ces tâches sont vraiment dépendantes à la langue traitée.
	Tellement elles sont simples, nous pouvons implémenter des outils pour n'importe quelle langue en sachant les règles morphologiques. 
	Malheureusement, pas toutes les langues fournissent de tels outils.  
	C'est la responsabilité des gens qui parlent ces langues à les développer dans un monde de plus en plus numérique.
	
%\end{discussion}

\sectioni{Ressources supplémentaires}
%\begin{ressources}
	
\subsubsection*{Exercices}

\begin{enumerate}
	\item Donner l'expression régulière qui cherche les mots ``\textbf{il}" (singulier et pluriel). 
	Comme par exemple : \expword{Il a passé par son lieu de travail et il a dit : ``j'ai devenu vieil ; ils n'ont plus besoin de moi".}
	\item Étant donné un log, on veut afficher les lignes qui commencent par ``\textbf{Error}" ; qui contiennent un nombre commençant par un chiffre autre que zéro, ayant des 3 à 5 zéros consécutifs et se terminant par un chiffre autre que zéro ; et qui se terminent par ``\textbf{...}"
	\item Donner l'expression régulière qui cherche les mots contenant les lettres ``l", ``i" et ``n" dans cet ordre. Le début du mot peut être en majuscule, le reste du mot est en minuscule. Par exemple, \expword{lion, Linux, violine, absolution, Aladin, ...}
	\item Afin d'écrire le mot "Rassemblement",  on a commis l'erreur suivante : "Rasenlbement". Indiquer la (les) position(s) de chaque opération d'édition par rapport au mot correcte (Si l'opération n'existe pas, écrire 0. La transposition et la substitution sont prioritaires, c-à-d. on ne doit pas les considérer comme des opérations d'insertion/suppression) :
	
	\begin{tabular}{|lll|}
		\hline 
		Insertion : ............ & & Substitution : ............ \\
		Suppression : ............ & & Transposition : ............ \\
		\hline
	\end{tabular}
	
	\item Calculer les distances de Hamming et de  Levenstein des deux mots suivants : ``tray" et ``tary".
	Indiquer les différentes opérations d'édition pour chaque distance. Refaire la même chose pour la distance de Lavenstein (poids de substitution = 1).
	
	\item Quelles sont les caractéristiques suffisantes pour détecter la limite des phrases dans le texte ``\textbf{I met Mr. Karim. He is at Hassiba Ave. He went to buy a PC.}" (Ave.=avenue [FR: rue]), étant donné qu'on a appliqué une phase de normalisation ? Cette dernière transforme les abréviations/acronymes vers leurs versions longues en gardant le point lorsque le mot suivant commence par une majuscule et l'abréviation peut se produire à la fin de la phrase (les caractéristiques utiles mais pas nécessaires dans ce texte sont considérées comme erronées) :
	
	\begin{tabular}{|lll|}
		\hline 
		\Square\ Casse (après ``.") & \Square\ Catégorie grammaticale (avant ``.") & \Square\ Longueur du mot (avant ``.")\\
		\Square\ Nom propre (avant ``.")& \Square\ Catégorie grammaticale (après ``.")& \Square\ Longueur du mot (après ``.")\\
		\Square\ Nom propre (après ``.") & \Square\ Classes des abréviations& \Square\ Aucune ; le ``." est suffisant\\
		\hline
	\end{tabular}

	\item Choisir, pour chaque tâche, l'opération (les opérations)  de réduction de forme utilisée(s) souvent (avec élimination des affixes) :
	
	\begin{tabular}{|llll|}
		\hline 
		Tâche & Lemmatisation & Racinisation & Aucune\\
		\hline
		Recherche d'informations (RI) & \Square & \Square & \Square \\
		Compréhension de la langue (NLU) & \Square & \Square & \Square \\
		\hline
	\end{tabular}
	
\end{enumerate}


\subsubsection*{Tutoriels}

Il existe plusieurs tutoriels accessibles à partir du répertoire Github (CH02).
Stanford CoreNLP est un outil de TALN qui supporte l'arabe, le chinois, l'anglais, le français, l'allemand et l'espagnol.
Il est implémenté en java où les tâches sont conçues sous formes des pipelines.
Dans ce tutoriel, nous présentons la segmentation et la lemmatisation en utilisant cet outil.

LangPi est un autre outil implémenté en java pour TALN.
Il a été développé par l'auteur de cet ouvrage principalement pour faciliter le prétraitement des textes.
Les tâches de prétraitement (normalisation, segmentation, radicalisation et filtrage des mots vides) sont accessibles via des interfaces facilitant l'ajout des nouvelles langues.
Actuellement, cet outil supporte plus de 40 langues.
Dans ce tutoriels, nous présentons comment pré-traiter un texte avec cet outil.

Apache OpenNLP est un outil implémenté en java pour TALN.
Il fournit des interfaces pour déférentes tâches où on peut entraîner un modèle pour une certaine langue.
Dans ce tutoriel, nous présentons le modèle de détection des langues : étant donnée un texte, on détecte sa langue.
Nous avons essayé d'entraîner un modèle pour différencier entre le langage binaire, décimal et hexadécimal. 
Le dataset d'entraînement est un peu petit ; donc, le modèle ne sera pas performant.
Ensuite, nous présentons la segmentation des phrases et des mots, ainsi que l'entraînement d'un modèle de segmentation des phrases.

NLTK est un outil très connu, présentant plusieurs tâches de TALN en python.
Dans ce tutoriel, nous présentons la segmentation des phrases, mots, syllabes et tweets.
L'outil supporte plusieurs langues pour les tâches de filtrages des mots vides, la radicalisation  et la lemmatisation. 
Ces tâches sont présentées en plus des différentes distances d'édition.


Spacy est un outil en python où les tâches sont représentées sous forme des pipelines.
Chaque langue présente des pipelines possibles qu'on stocke sous forme d'un modèle après entraînement.
Dans ce tutoriel, nous utilisons le modèle de l'anglais pour la segmentation (phrases et mots), le filtrage des mots vides et la lemmatisation.



\subsubsection*{TP : fouille des contacts}

On veut récupérer les adresses émail, les réseaux sociaux et les numéros de téléphone qui se figurent dans les pages "contactez nous" des sites web en Algérie. 
Le problème est que ces pages représentent ces informations de plusieurs façons.
Pour récupérer et unifier la forme de ces informations, on va utiliser les expressions régulières. 

L'énoncé complet du TP ainsi que les codes et les données sont téléchargeables à partir du répertoire Github.
Le TP est implémenté complètement à partir de zéro (from scratch) : recherche des contactes, évaluation, etc. 
L'étudiant doit seulement introduire des expressions régulières afin d'améliorer le score F1.
Les langages de programmation disponibles (pour l'instant) sont : Java, Javascript/nodejs et Python.


%\end{ressources}

%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%===================================================================== 
